{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "from torch.utils import data\n",
    "from torch.cuda import amp\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gc #垃圾回收\n",
    "\n",
    "from tqdm import tqdm #进度条 for data in tqdm(range(100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"E:/DATA/feedback-prize-effectiveness/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   discourse_id      essay_id  \\\n",
      "0  0013cc385424  007ACE74B050   \n",
      "1  9704a709b505  007ACE74B050   \n",
      "2  c22adee811b6  007ACE74B050   \n",
      "3  a10d361e54e4  007ACE74B050   \n",
      "4  db3e453ec4e2  007ACE74B050   \n",
      "5  36a565e45db7  007ACE74B050   \n",
      "6  fb65fe816ba3  007ACE74B050   \n",
      "7  4e472e2584fa  007ACE74B050   \n",
      "8  28a94d3ee425  007ACE74B050   \n",
      "9  d226f06362f5  00944C693682   \n",
      "\n",
      "                                      discourse_text        discourse_type  \\\n",
      "0  Hi, i'm Isaac, i'm going to be writing about h...                  Lead   \n",
      "1  On my perspective, I think that the face is a ...              Position   \n",
      "2  I think that the face is a natural landform be...                 Claim   \n",
      "3  If life was on Mars, we would know by now. The...              Evidence   \n",
      "4  People thought that the face was formed by ali...          Counterclaim   \n",
      "5  though some say that life on Mars does exist, ...              Rebuttal   \n",
      "6  It says in paragraph 7, on April 5, 1998, Mars...              Evidence   \n",
      "7  Everyone who thought it was made by alieans ev...          Counterclaim   \n",
      "8  Though people were not satified about how the ...  Concluding Statement   \n",
      "9  Limiting the usage of cars has personal and pr...                  Lead   \n",
      "\n",
      "  discourse_effectiveness  \n",
      "0                Adequate  \n",
      "1                Adequate  \n",
      "2                Adequate  \n",
      "3                Adequate  \n",
      "4                Adequate  \n",
      "5             Ineffective  \n",
      "6                Adequate  \n",
      "7                Adequate  \n",
      "8                Adequate  \n",
      "9               Effective  \n",
      "Hi, i'm Isaac, i'm going to be writing about how this face on Mars is a natural landform or if there is life on Mars that made it. The story is about how NASA took a picture of Mars and a face was seen on the planet. NASA doesn't know if the landform was created by life on Mars, or if it is just a natural landform. \n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(data_dir+\"train.csv\")\n",
    "print(df1.head(10))\n",
    "print(df1.iloc[0]['discourse_text'])\n",
    "#essay_id   txt文件名\n",
    "#discourse_id   段落的id，应该没用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   discourse_id      essay_id  \\\n",
      "0  a261b6e14276  D72CB1C11673   \n",
      "1  5a88900e7dc1  D72CB1C11673   \n",
      "2  9790d835736b  D72CB1C11673   \n",
      "3  75ce6d68b67b  D72CB1C11673   \n",
      "4  93578d946723  D72CB1C11673   \n",
      "\n",
      "                                      discourse_text discourse_type  \n",
      "0  Making choices in life can be very difficult. ...           Lead  \n",
      "1  Seeking multiple opinions can help a person ma...       Position  \n",
      "2                     it can decrease stress levels           Claim  \n",
      "3             a great chance to learn something new           Claim  \n",
      "4               can be very helpful and beneficial.           Claim  \n",
      "Making choices in life can be very difficult. People often ask for advice when they can not decide on one thing. It's always good to ask others for their advice when making a choice. When you have multiple opinions you have the ability to make the best choice for yourself. \n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(data_dir+\"test.csv\")\n",
    "print(df2.head())\n",
    "print(df2.iloc[0]['discourse_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear: Principal\n",
      "\n",
      "I am arguing against the policy change because even though there are some children out there that really needs help with their academic work, that does not mean that only because they have a c average that would not let them enjoy their sports or other activities unless they've a B average.\n",
      "\n",
      "Sometimes teachers or even principal needs to consider that we should give the help that any student should have. Also this may consider student self as steam. Meaning student would start to feel sad nervous, and not wanting to go to school because of the reason they have a low averages and they can not participate in other activities or sports. The fact that there are children that would want to enjoy many good things the school is actually giving it to them.\n",
      "\n",
      "We would want to make changes as, \"like to be a better person for a better tomorrow\" This supports the idea of having have many good thoughts and incasing your work as much as possible. In some situation like arguing we should make a vote to see if kids would want to have a school policy of change and having to participate in fun activities but they first need to have at least a B average. Some reasons I would be against the school policy change is because you would feel ashame and then many bad things could happen meaning, you would be angry etc. In additionally , I think that student should have sports because it helps them with their health, and problems that they would have personal. I am against this policy change.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "Student.\n",
      "-------\n",
      "Dear Florida senator,\n",
      "\n",
      "I would like disuss why changing up the way we voyte today is not as bad f an idea as it sounds. Changing to elections by a popular vote sounds like its in the peoples hands instead of juts casting out votes on the slate electors. Aren't we voting for our president. innstead of countting onn the electoral votes to hep out or, go against out favor, it'd be much more easier to just let the people vote.\n",
      "\n",
      "The elctoral votes are utterly useless or unimportant,though. If there is ever happens to be a tie between the two candidates the electoral votes can help break it. It'd be less of a hassle to have the florida residents vote,directly, on someone besides the president. Less worring about if theyre going to win or not. Now when you start to think about it the electoral college is just unfair an pretty outdated,if you ask me. Change doesn't sound that bad. All im trying to achive by writting this, is to gain actual control over who we're all voting for.\n",
      "\n",
      "It's also not just what's completely wrong with the electoral college but how does it help in the first place? When you start analizing it, it really doens't help much. How much actual help can the lectora coellge make, in my defense not much. Why? Simple, its just 20-30 more votes and not many states have that much Hawaii has uop to 4 electoral votes, not much. Voters should be alowed to directly vote for who they want to be president and not rely on the lectoral college.    \n"
     ]
    }
   ],
   "source": [
    "with open(data_dir+\"/train/000E6DE9E817.txt\") as f:\n",
    "    print(f.read())\n",
    "print(\"-------\")\n",
    "with open(data_dir+\"/train/00B144412785.txt\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "## 数据基本处理\n",
    "1. 一般语言处理中对全部数据只会取常用的n个词，在此之外的词是不认识的，即先有一本字典\n",
    "2. 用one-hot编码时，每个词都表示为长n的向量，其中只有一个值是1，其余全是0。比如字典的第i（0开始）个单词其编码中1值的下标是i\n",
    "\n",
    "## embedding\n",
    "1. one_hot编码稀疏，所以考虑用稠密向量表示词，比如只用长为m<n的向量表示n个单词，其中单词间关系可以体现在向量间关系中，比如$\\vec{男}+\\vec{国王}=\\vec{皇帝}$\n",
    "2. 可由embedding层实现这个，embedding记录了一个(n,m)的矩阵，每行都是一个单词的稠密向量，作用是one-hot编码的向量按照其1值的下标i访问这个矩阵第i行，取出这行向量作为新输入\n",
    "3. 具体使用看下面代码说明\n",
    "4. embedding可由自行训练出，也可预加载预训练参数。使用预训练参数时，冻结此层\n",
    "\n",
    "## 初步结果\n",
    "将batch_size\\*len_sentences\\*n变为batch_size\\*len_sentences\\*m\n",
    "\n",
    "## rnn具体流程\n",
    "1. 首先初始化hadden_input为全0\n",
    "2. 对每个词，其都会和当前的hadden_input一起进入网络(cat或add)进行一步（liner、tanh激活）运算，所得的输出作为新的hadden_input与下一个词的向量一起进入网络（cat或add）\n",
    "3. cat（最后一维）的话，需要截断，或者另外卷积一次获得新hadden_input\n",
    "4. pytorch的rnn为x[i]通过一liner，hidden通过一liner，两个结果相加经Tanh激活，结果作为x[i]和新hidden，具体见下面代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before embedding:torch.Size([4, 20])\n",
      "after embedding:torch.Size([4, 20, 8])\n",
      "-------------\n",
      "rnn input:torch.Size([20, 4, 8])\n",
      "rnn out:torch.Size([20, 4, 128])\n",
      "rnn outh:torch.Size([4, 128])\n",
      "-----------\n",
      "finally shape:torch.Size([4, 2])\n",
      "tensor([[0.5058, 0.4942],\n",
      "        [0.5616, 0.4384],\n",
      "        [0.4322, 0.5678],\n",
      "        [0.4265, 0.5735]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# rnn内部细节\n",
    "batch_size = 4\n",
    "sentence_len = 20 #一句话20个词\n",
    "words_num = 100 #字典记录了100个词\n",
    "words_len = 8 #字典的每个词向量长8\n",
    "X = torch.randint(0,words_num,[batch_size,sentence_len]) #注意输入不为one-hot，只是每个词的字典序号，比如[3,2,10]表示一句话。int\n",
    "y = torch.as_tensor([[0.,1],[0,1],[1,0],[1,0]])\n",
    "#print(X)\n",
    "\n",
    "hidden_layer_num = 1 #多少个hidden用于循环，即多少个循环部分\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = 128\n",
    "        self.embedding_layer = nn.Embedding(words_num,words_len,padding_idx=0) #100*8,padding_idx为输入长度不够时填充的字典词序号\n",
    "\n",
    "        #一个循环节\n",
    "        self.hidden_layer_x = nn.Linear(words_len,self.hidden_size) #rnn关键部分\n",
    "        self.hidden_layer_h = nn.Linear(self.hidden_size,self.hidden_size) #给h用\n",
    "\n",
    "        #分类器\n",
    "        self.out_layer = nn.Linear(self.hidden_size,2)\n",
    "        self.activation_layer = nn.Softmax(dim=-1) #dim=0表示a[i][j][k]按i方向的几个数一起算\n",
    "\n",
    "    def __init_hidden(self):\n",
    "        return torch.zeros([hidden_layer_num,batch_size,self.hidden_size])\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.hidden = self.__init_hidden()\n",
    "\n",
    "        print(f\"before embedding:{x.shape}\") #[4, 20]\n",
    "        x = self.embedding_layer(x)\n",
    "        print(f\"after embedding:{x.shape}\") #[4, 20, 8]\n",
    "        print(\"-------------\")\n",
    "\n",
    "        out = torch.zeros([sentence_len, batch_size, self.hidden_size])\n",
    "\n",
    "        #rnn部分\n",
    "        # 为了更好计算，将数据x变形为为len_sencentces*batch_size*words_num\n",
    "        # 即x[0]为各句子首单词\n",
    "        x = x.transpose(0,1)\n",
    "        print(f\"rnn input:{x.shape}\") #[20, 4, 128]\n",
    "        for i in range(x.shape[0]):\n",
    "            a1 = self.hidden_layer_x(x[i])\n",
    "            a2 = self.hidden_layer_h(self.hidden[0])\n",
    "\n",
    "            out[i] = self.hidden[0] = nn.Tanh()(a1+a2)\n",
    "        print(f\"rnn out:{out.shape}\")\n",
    "        print(f\"rnn outh:{self.hidden[0].shape}\")\n",
    "        print(\"-----------\")\n",
    "        \n",
    "        #分类器\n",
    "        o = self.hidden[0]\n",
    "        o = self.out_layer(o)\n",
    "        o = self.activation_layer(o)\n",
    "        print(f\"finally shape:{o.shape}\") #[4, 2]\n",
    "        print(o)\n",
    "\n",
    "        return o\n",
    "\n",
    "rnn = MyRNN()\n",
    "out = rnn(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch的rnn\n",
    "- 首先就是输入是batch_size\\*len_sencentces\\*words_num  \n",
    "所以Embedding算是预处理部分，如果需要训练则？？？？？？？？\n",
    "- 主要公式$$h_t = \\tanh(x_t W_{ih}^T + b_{ih} + h_{t-1}W_{hh}^T + b_{hh})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 8])\n",
      "torch.Size([2, 5, 32]) torch.Size([1, 2, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "sentence_len = 5\n",
    "words_num = 10\n",
    "words_len = 8\n",
    "\n",
    "X = torch.randint(0,words_num,[batch_size,sentence_len])\n",
    "\n",
    "X = nn.Embedding(words_num,words_len)(X)\n",
    "#X = X.transpose(0,1) #batch_size放在第二维，则batch_first设置为False\n",
    "print(X.shape)\n",
    "\n",
    "hidden_size=32\n",
    "num_layers=1\n",
    "H = torch.zeros([num_layers,batch_size,hidden_size]) #可以不设置，则默认为0，这是单向rnn\n",
    "#HH = torch.zeros([num_layers*2,batch_size,hidden_size]) #双向rnn，需要RNN中设置bidirectional=True\n",
    "\n",
    "rnn = nn.RNN(\n",
    "            input_size=words_len,\n",
    "\n",
    "            #hidden_size，num_layers 都是对网络的设置，与输入数据无关，设置相对自由\n",
    "            hidden_size=hidden_size,     # hidden层大小\n",
    "            num_layers=num_layers,       # n个rnn层\n",
    "\n",
    "            batch_first=True, #True则输入输出的batch在第一维，否则在第二维（参照上面MyRNN在hidden前的变形）\n",
    "\n",
    "            bidirectional=False, #是否双向rnn\n",
    "        )\n",
    "\n",
    "out,outh = rnn(X,H) #out为h的集合\n",
    "print(out.shape,outh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([32, 8]) <class 'torch.nn.parameter.Parameter'>\n",
      "weight_hh_l0 torch.Size([32, 32]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias_ih_l0 torch.Size([32]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias_hh_l0 torch.Size([32]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for i in rnn.named_parameters():\n",
    "    print(i[0],i[1].shape,type(i[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 4])\n",
      "pytorch部分\n",
      "torch.Size([2, 5, 8]) torch.Size([1, 2, 8])\n",
      "tensor([-0.7137,  0.5926, -0.6244,  0.6198, -0.1706, -0.5207, -0.0612,  0.4735],\n",
      "       grad_fn=<SliceBackward0>) \n",
      " tensor([-0.8456,  0.2111,  0.1764,  0.7712, -0.0146, -0.7987, -0.6467,  0.7652],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "-------------------\n",
      "MyCNN部分\n",
      "torch.Size([2, 5, 8]) torch.Size([1, 2, 8])\n",
      "tensor([-0.7137,  0.5926, -0.6244,  0.6198, -0.1706, -0.5207, -0.0612,  0.4735],\n",
      "       grad_fn=<SliceBackward0>) \n",
      " tensor([-0.8456,  0.2111,  0.1764,  0.7712, -0.0146, -0.7987, -0.6467,  0.7652],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1024)\n",
    "\n",
    "batch_size = 2\n",
    "sentence_len = 5\n",
    "words_num = 10\n",
    "words_len = 4\n",
    "\n",
    "hidden_size=8\n",
    "num_layers=1\n",
    "\n",
    "X = torch.randint(0,words_num,(batch_size,sentence_len))\n",
    "em = nn.Embedding(words_num,words_len)\n",
    "X = em(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('pytorch部分')\n",
    "rnn = nn.RNN(\n",
    "            input_size=words_len,\n",
    "\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "\n",
    "            batch_first=True,\n",
    "        )\n",
    "out,outh = rnn(X)\n",
    "print(out.shape,outh.shape)\n",
    "print(outh[0,0,:],'\\n',out[0,0,:])\n",
    "print('-------------------')\n",
    "\n",
    "\n",
    "print('MyCNN部分')\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.hidden_layer_x = nn.Linear(words_len,hidden_size) #rnn关键部分\n",
    "        self.hidden_layer_h = nn.Linear(hidden_size,hidden_size) #h用\n",
    "\n",
    "        #注意这里无条件复制w、b，所以最好先判断两者形状相等再赋值\n",
    "        params = list(rnn.parameters())\n",
    "        assert self.hidden_layer_x.weight.shape == params[0].shape and \\\n",
    "                self.hidden_layer_h.weight.shape == params[1].shape and \\\n",
    "                self.hidden_layer_x.bias.shape == params[2].shape and \\\n",
    "                self.hidden_layer_h.bias.shape == params[3].shape,\\\n",
    "                print(\"shape error\")\n",
    "\n",
    "        self.hidden_layer_x.weight = params[0]\n",
    "        self.hidden_layer_h.weight = params[1]\n",
    "        self.hidden_layer_x.bias = params[2]\n",
    "        self.hidden_layer_h.bias = params[3]\n",
    "\n",
    "    def __init_hidden(self):\n",
    "        return torch.zeros([num_layers,batch_size,hidden_size])\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.hidden = self.__init_hidden()\n",
    "\n",
    "        out = torch.zeros([sentence_len,batch_size,hidden_size])\n",
    "\n",
    "        x = x.transpose(0,1)\n",
    "        for i in range(x.shape[0]):\n",
    "            a1 = self.hidden_layer_x(x[i])\n",
    "            a2 = self.hidden_layer_h(self.hidden[0])\n",
    "            self.hidden[0] = nn.Tanh()(a1+a2)\n",
    "\n",
    "\n",
    "            out[i] = self.hidden[0]\n",
    "\n",
    "        return out.transpose(0,1)\n",
    "\n",
    "myrnn = MyRNN()\n",
    "out = myrnn(X)\n",
    "print(out.shape,myrnn.hidden.shape)\n",
    "print(myrnn.hidden[0,0,:],'\\n',out[0,0,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "## 基本理解\n",
    "- rnn因为激活层是Tanh，显然，当前信息几乎不会对很远的计算产生影响，即只能短期记忆\n",
    "- lstm中引入可以选择“记忆”和“当前”的信息对当前输出的占比 \n",
    "## 具体介绍 \n",
    "- 输入是H（类似于rnn的hidden作用），C（当前记忆）\n",
    "- 一个单元分为记忆门，遗忘门，rnn门，输出门，其相当于四次rnn变换，只不过功能和激活函数不同\n",
    "   - 记忆门i，Sigmoid激活，即当前rnn输出需要记哪些信息到记忆C中\n",
    "   - 遗忘门f，Sigmoid激活，即原先记忆需要遗忘哪些信息\n",
    "   - rnn门g，Tanh激活，就是rnn\n",
    "   - 输出门o，Sigmoid激活，控制新记忆哪些作为新H\n",
    "- 公式如下\n",
    "$$\n",
    "    \\begin{array}{ll} \\\\\n",
    "        i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "        f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "        o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "        c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "        h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "    \\end{array}\n",
    "$$\n",
    "## pytorch内部细节\n",
    "- 将四个变换按维度一放在一起，同时计算四个门激活前的输出，输出顺序如上顺序，见如下代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 4])\n",
      "pytorch部分\n",
      "torch.Size([2, 5, 16]) torch.Size([1, 2, 16]) torch.Size([1, 2, 16])\n",
      "tensor([ 0.0842,  0.0372, -0.1389,  0.0300, -0.1425,  0.0223,  0.0017,  0.3239,\n",
      "        -0.0413, -0.0924, -0.1835,  0.0621,  0.0484, -0.0894,  0.2133,  0.0063],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "-------------------\n",
      "MyLSTM部分\n",
      "torch.Size([2, 5, 16]) torch.Size([1, 2, 16]) torch.Size([1, 2, 16])\n",
      "tensor([ 0.0842,  0.0372, -0.1389,  0.0300, -0.1425,  0.0223,  0.0017,  0.3239,\n",
      "        -0.0413, -0.0924, -0.1835,  0.0621,  0.0484, -0.0894,  0.2133,  0.0063],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1024)\n",
    "\n",
    "batch_size = 2\n",
    "sentence_len = 5\n",
    "words_num = 10\n",
    "words_len = 4\n",
    "\n",
    "hidden_size=16\n",
    "num_layers=1\n",
    "\n",
    "X = torch.randint(0,words_num,(batch_size,sentence_len))\n",
    "em = nn.Embedding(words_num,words_len)\n",
    "X = em(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('pytorch部分')\n",
    "lstm = nn.LSTM(\n",
    "            input_size=words_len,\n",
    "\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "\n",
    "            batch_first=True,\n",
    "        )\n",
    "H = torch.zeros([num_layers,batch_size,hidden_size])\n",
    "C = torch.zeros([num_layers,batch_size,hidden_size])\n",
    "out,(outh,outc) = lstm(X,(H,C)) #H、C默认为0\n",
    "print(out.shape,outh.shape,outc.shape)\n",
    "print(out[0,0,:])\n",
    "print('-------------------')\n",
    "\n",
    "\n",
    "print('MyLSTM部分')\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layer_x = nn.Linear(words_len,hidden_size*4) #4*32，即同时输出4组\n",
    "        self.hidden_layer_h = nn.Linear(hidden_size,hidden_size*4) #8*32\n",
    "\n",
    "        params = list(lstm.parameters())\n",
    "        assert self.hidden_layer_x.weight.shape == params[0].shape and \\\n",
    "                self.hidden_layer_h.weight.shape == params[1].shape and \\\n",
    "                self.hidden_layer_x.bias.shape == params[2].shape and \\\n",
    "                self.hidden_layer_h.bias.shape == params[3].shape, \\\n",
    "                print(\"shape error\")\n",
    "\n",
    "        self.hidden_layer_x.weight = params[0]\n",
    "        self.hidden_layer_h.weight = params[1]\n",
    "        self.hidden_layer_x.bias = params[2]\n",
    "        self.hidden_layer_h.bias = params[3]\n",
    "\n",
    "    def __init_H_C(self):\n",
    "        return torch.zeros([num_layers,batch_size,hidden_size]),torch.zeros([num_layers,batch_size,hidden_size])\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.H,self.C = self.__init_H_C()\n",
    "        #H是输出\n",
    "        #C是记忆\n",
    "\n",
    "        out = torch.zeros([sentence_len,batch_size,hidden_size])\n",
    "\n",
    "        x = x.transpose(0,1)\n",
    "        for i in torch.arange(x.shape[0]):\n",
    "\n",
    "            a1 = self.hidden_layer_x(x[i])\n",
    "            a2 = self.hidden_layer_h(self.H[0])\n",
    "            a = a1+a2\n",
    "\n",
    "            remember_gate = nn.Sigmoid()(a[:,:hidden_size*1]) #记住下面op什么信息\n",
    "            forget_gate = nn.Sigmoid()(a[:,hidden_size*1:hidden_size*2]) #C遗忘什么信息\n",
    "            op = nn.Tanh()(a[:,hidden_size*2:hidden_size*3])\n",
    "            output_gate = nn.Sigmoid()(a[:,hidden_size*3:]) #最终输出\n",
    "\t\t\t\n",
    "            self.C[0] = self.C[0]*forget_gate + remember_gate*op #新的记忆\n",
    "            self.H[0] = nn.Tanh()(self.C)*output_gate #生成新的输出\n",
    "\n",
    "            out[i] = self.H[0]\n",
    "\n",
    "        return out.transpose(0,1)\n",
    "\n",
    "mylstm = MyLSTM()\n",
    "out = mylstm(X)\n",
    "print(out.shape,mylstm.C.shape,mylstm.H.shape)\n",
    "print(out[0,0,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化\n",
    "- 遗忘门=1-记忆门，减少运算次数\n",
    "- 。。。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4115338fe7ab4a538fea3ef25fde7a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\project\\python\\feedback-prize-effectiveness\\exp.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m question_answerer \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mquestion-answering\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m#内部有模型，用于回答问题的\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mExtractive Question Answering is the task of extracting an answer from a text given a question. An example of a\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mquestion answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39ma model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m question_answerer(question\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWhat is extractive question answering?\u001b[39m\u001b[39m\"\u001b[39m, context\u001b[39m=\u001b[39mcontext)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\pipelines\\__init__.py:724\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    723\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 724\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    725\u001b[0m     model,\n\u001b[0;32m    726\u001b[0m     model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    727\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    728\u001b[0m     framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    729\u001b[0m     task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    730\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    731\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    732\u001b[0m )\n\u001b[0;32m    734\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    735\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\pipelines\\base.py:257\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m     )\n\u001b[0;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 257\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39mfrom_pretrained(model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    258\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    259\u001b[0m         model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    462\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    465\u001b[0m     )\n\u001b[0;32m    466\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py:2137\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2123\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2124\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m   2125\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2126\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2135\u001b[0m         _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   2136\u001b[0m     )\n\u001b[1;32m-> 2137\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   2139\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2140\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m SAFE_WEIGHTS_NAME:\n\u001b[0;32m   2142\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[0;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[0;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[0;32m    122\u001b[0m     )\n\u001b[1;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:1242\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[0;32m   1240\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[1;32m-> 1242\u001b[0m     http_get(\n\u001b[0;32m   1243\u001b[0m         url_to_download,\n\u001b[0;32m   1244\u001b[0m         temp_file,\n\u001b[0;32m   1245\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1246\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1247\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1248\u001b[0m     )\n\u001b[0;32m   1250\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, blob_path)\n\u001b[0;32m   1251\u001b[0m _chmod_and_replace(temp_file\u001b[39m.\u001b[39mname, blob_path)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:495\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[0;32m    486\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    487\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    488\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    489\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    494\u001b[0m )\n\u001b[1;32m--> 495\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[0;32m    496\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    497\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py:627\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 627\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    629\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    630\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py:566\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    563\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    565\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 566\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py:532\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    530\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\") #内部有模型，用于回答问题的\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\"\n",
    "question_answerer(question=\"What is extractive question answering?\", context=context)\n",
    "\n",
    "#中文版\n",
    "from transformers import AutoModelForQuestionAnswering,AutoTokenizer,pipeline\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('uer/roberta-base-chinese-extractive-qa')\n",
    "tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-chinese-extractive-qa')\n",
    "zh_qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "QA_input = {\n",
    "    'question': \"著名诗歌《假如生活欺骗了你》的作者是\",\n",
    "    'context': \"普希金从那里学习人民的语言，吸取了许多有益的养料，\\\n",
    "        这一切对普希金后来的创作产生了很大的影响。这两年里，普希金创作了不少优秀的作品，如《囚徒》、\\\n",
    "            《致大海》、《致凯恩》和《假如生活欺骗了你》等几十首抒情诗，叙事诗《努林伯爵》，历史剧\\\n",
    "                《鲍里斯·戈都诺夫》，以及《叶甫盖尼·奥涅金》前六章。\"\n",
    "                }\n",
    "zh_qa(QA_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你', '好', '吗', '，', '吃', '饭', '了', '吗', '?']\n",
      "{'input_ids': [[101, 5031, 3428, 3221, 679, 7444, 6206, 102, 0, 0, 0, 0, 0, 0, 0], [101, 2130, 1059, 679, 7444, 6206, 818, 862, 7583, 1912, 3082, 868, 102, 0, 0], [101, 1914, 3340, 3144, 2945, 1469, 1296, 3340, 3144, 2945, 671, 3416, 6822, 6121, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# input_ids: 各字符编号，其中101表开始，102表句子结束[SEP]，0为补齐\n",
    "# attention_mask: 补齐后末尾有0.用此指示哪些是有用信息\n",
    "# token_type_ids: 有时处理问答问题，则输入是两句，用此表示两句，其中第一句全0，第二句全一，若还有第三句则又全0\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\") #https://huggingface.co/有很多模型\n",
    "sen = \"你好吗，吃饭了吗?\"\n",
    "tokens = tokenizer.tokenize(sen)\n",
    "print(tokens)\n",
    "\n",
    "sens = [\"答案是不需要\",\"完全不需要任何额外操作\",\"多条数据和单条数据一样进行调用即可.\"]\n",
    "res = tokenizer(\n",
    "    sens, \n",
    "    padding=\"max_length\", #不足补齐\n",
    "    max_length=15,\n",
    "    truncation=True #超过截断\n",
    "    )\n",
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasets\n",
    "来自hugging官网"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acronym_identification',\n",
       " 'ade_corpus_v2',\n",
       " 'adversarial_qa',\n",
       " 'aeslc',\n",
       " 'afrikaans_ner_corpus',\n",
       " 'ag_news',\n",
       " 'ai2_arc',\n",
       " 'air_dialogue',\n",
       " 'ajgt_twitter_ar',\n",
       " 'allegro_reviews']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#可下载数据集\n",
    "from datasets import list_datasets\n",
    "list_datasets()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration madao33--new-title-chinese-2423910db071caac\n",
      "Found cached dataset csv (C:/Users/zjt/.cache/huggingface/datasets/madao33___csv/madao33--new-title-chinese-2423910db071caac/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31d9d81806f433c9349af1b352ce784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#下载数据集\n",
    "from datasets import load_dataset\n",
    "dt= load_dataset(\"madao33/new-title-chinese\") #一个数据集\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 5850\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '望海楼美国打“台湾牌”是危险的赌博',\n",
       " 'content': '近期，美国国会众院通过法案，重申美国对台湾的承诺。对此，中国外交部发言人表示，有关法案严重违反一个中国原则和中美三个联合公报规定，粗暴干涉中国内政，中方对此坚决反对并已向美方提出严正交涉。\\n事实上，中国高度关注美国国内打“台湾牌”、挑战一中原则的危险动向。近年来，作为“亲台”势力大本营的美国国会动作不断，先后通过“与台湾交往法”“亚洲再保证倡议法”等一系列“挺台”法案，“2019财年国防授权法案”也多处触及台湾问题。今年3月，美参院亲台议员再抛“台湾保证法”草案。众院议员继而在4月提出众院版的草案并在近期通过。上述法案的核心目标是强化美台关系，并将台作为美“印太战略”的重要伙伴。同时，“亲台”议员还有意制造事端。今年2月，5名共和党参议员致信众议院议长，促其邀请台湾地区领导人在国会上发表讲话。这一动议显然有悖于美国与台湾的非官方关系，其用心是实质性改变美台关系定位。\\n上述动向出现并非偶然。在中美建交40周年之际，两国关系摩擦加剧，所谓“中国威胁论”再次沉渣泛起。美国对华认知出现严重偏差，对华政策中负面因素上升，保守人士甚至成立了“当前中国威胁委员会”。在此背景下，美国将台海关系作为战略抓手，通过打“台湾牌”在双边关系中增加筹码。特朗普就任后，国会对总统外交政策的约束力和塑造力加强。其实国会推动通过涉台法案对行政部门不具约束力，美政府在2018年并未提升美台官员互访级别，美军舰也没有“访问”台湾港口，保持着某种克制。但从美总统签署国会通过的法案可以看出，国会对外交产生了影响。立法也为政府对台政策提供更大空间。\\n然而，美国需要认真衡量打“台湾牌”成本。首先是美国应对危机的代价。美方官员和学者已明确发出警告，美国卷入台湾问题得不偿失。美国学者曾在媒体发文指出，如果台海爆发危机，美国可能需要“援助”台湾，进而导致新的冷战乃至与中国大陆的冲突。但如果美国让台湾自己面对，则有损美国的信誉，影响美盟友对同盟关系的支持。其次是对中美关系的危害。历史证明，中美合则两利、斗则两伤。中美关系是当今世界最重要的双边关系之一，保持中美关系的稳定发展，不仅符合两国和两国人民的根本利益，也是国际社会的普遍期待。美国蓄意挑战台湾问题的底线，加剧中美关系的复杂性和不确定性，损害两国在重要领域合作，损人又害己。\\n美国打“台湾牌”是一场危险的赌博。台湾问题是中国核心利益，中国政府和人民决不会对此坐视不理。中国敦促美方恪守一个中国原则和中美三个联合公报规定，阻止美国会审议推进有关法案，妥善处理涉台问题。美国悬崖勒马，才是明智之举。\\n（作者系中国国际问题研究院国际战略研究所副所长）'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 5265\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 585\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#自行分割数据集\n",
    "dt1 = dt[\"train\"]\n",
    "dt1 = dt1.train_test_split(test_size=0.1)\n",
    "dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#选取过滤数据\n",
    "# 选取\n",
    "dt[\"train\"].select([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\zjt\\.cache\\huggingface\\datasets\\madao33___csv\\madao33--new-title-chinese-2423910db071caac\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-abe034542a196555.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 544\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 过滤\n",
    "dt[\"train\"].filter(lambda example: \"中国\" in example[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eab1938edba48929745b0f63a0d2545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5850 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611ed5dbbd534c77b66cda10fa09401e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1679 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Prefix: 望海楼美国打“台湾牌”是危险的赌博'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map\n",
    "def add_prefix(example):\n",
    "    example[\"title\"] = 'Prefix: ' + example[\"title\"]\n",
    "    return example\n",
    "dt1 = dt.map(add_prefix)\n",
    "dt1[\"train\"][0][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1221cc575b64178b41c65db75debb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a904b87d4f404a4c90f0688764985d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实际中一般如下使用\n",
    "from transformers import AutoTokenizer\n",
    "tkzr = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "def preprocess_function(example):\n",
    "    model_inputs = tkzr(example[\"content\"], max_length=512, truncation=True)\n",
    "    labels = tkzr(example[\"title\"], max_length=32, truncation=True)\n",
    "    # label就是title编码的结果\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "dt1 = dt.map(preprocess_function,batched=True) #批处理\n",
    "dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'近期，美国国会众院通过法案，重申美国对台湾的承诺。对此，中国外交部发言人表示，有关法案严重违反一个中国原则和中美三个联合公报规定，粗暴干涉中国内政，中方对此坚决反对并已向美方提出严正交涉。\\n事实上，中国高度关注美国国内打“台湾牌”、挑战一中原则的危险动向。近年来，作为“亲台”势力大本营的美国国会动作不断，先后通过“与台湾交往法”“亚洲再保证倡议法”等一系列“挺台”法案，“2019财年国防授权法案”也多处触及台湾问题。今年3月，美参院亲台议员再抛“台湾保证法”草案。众院议员继而在4月提出众院版的草案并在近期通过。上述法案的核心目标是强化美台关系，并将台作为美“印太战略”的重要伙伴。同时，“亲台”议员还有意制造事端。今年2月，5名共和党参议员致信众议院议长，促其邀请台湾地区领导人在国会上发表讲话。这一动议显然有悖于美国与台湾的非官方关系，其用心是实质性改变美台关系定位。\\n上述动向出现并非偶然。在中美建交40周年之际，两国关系摩擦加剧，所谓“中国威胁论”再次沉渣泛起。美国对华认知出现严重偏差，对华政策中负面因素上升，保守人士甚至成立了“当前中国威胁委员会”。在此背景下，美国将台海关系作为战略抓手，通过打“台湾牌”在双边关系中增加筹码。特朗普就任后，国会对总统外交政策的约束力和塑造力加强。其实国会推动通过涉台法案对行政部门不具约束力，美政府在2018年并未提升美台官员互访级别，美军舰也没有“访问”台湾港口，保持着某种克制。但从美总统签署国会通过的法案可以看出，国会对外交产生了影响。立法也为政府对台政策提供更大空间。\\n然而，美国需要认真衡量打“台湾牌”成本。首先是美国应对危机的代价。美方官员和学者已明确发出警告，美国卷入台湾问题得不偿失。美国学者曾在媒体发文指出，如果台海爆发危机，美国可能需要“援助”台湾，进而导致新的冷战乃至与中国大陆的冲突。但如果美国让台湾自己面对，则有损美国的信誉，影响美盟友对同盟关系的支持。其次是对中美关系的危害。历史证明，中美合则两利、斗则两伤。中美关系是当今世界最重要的双边关系之一，保持中美关系的稳定发展，不仅符合两国和两国人民的根本利益，也是国际社会的普遍期待。美国蓄意挑战台湾问题的底线，加剧中美关系的复杂性和不确定性，损害两国在重要领域合作，损人又害己。\\n美国打“台湾牌”是一场危险的赌博。台湾问题是中国核心利益，中国政府和人民决不会对此坐视不理。中国敦促美方恪守一个中国原则和中美三个联合公报规定，阻止美国会审议推进有关法案，妥善处理涉台问题。美国悬崖勒马，才是明智之举。\\n（作者系中国国际问题研究院国际战略研究所副所长）'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1['train'][0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 6818, 3309, 8024, 5401]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1['train'][0]['input_ids'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1['train'][0]['token_type_ids'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1['train'][0]['attention_mask'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 3307, 3862, 3517, 5401]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1['train'][0]['labels'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d768252d973b4a6f85f2261d10b11eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c54144c6d943ab8ee3de5e17cca634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\datasets\\dataset_dict.py:1241: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存处理完的数据\n",
    "from datasets import load_from_disk\n",
    "dt.save_to_disk(\"./news_data\")\n",
    "dt = load_from_disk(\"./news_data\")\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cc9a47e40b59d786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/zjt/.cache/huggingface/datasets/csv/default-cc9a47e40b59d786/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f7aa8981f94ccda746d1e2466f0c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84864a888ac24d26badb305260850150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ba704cbdc54b37b8c5bc4e11fdfc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/zjt/.cache/huggingface/datasets/csv/default-cc9a47e40b59d786/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3876888866904d37852d01fa9e111619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'discourse_id', 'discourse_start', 'discourse_end', 'discourse_text', 'discourse_type', 'discourse_type_num', 'predictionstring'],\n",
       "        num_rows: 144293\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 本地数据集加载\n",
    "dt = load_dataset(\"csv\", data_files='E:\\\\DATA\\\\feedback-prize-2021\\\\train.csv')\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '423A1CA112E2',\n",
       " 'discourse_id': 1622627660524.0,\n",
       " 'discourse_start': 8.0,\n",
       " 'discourse_end': 229.0,\n",
       " 'discourse_text': 'Modern humans today are always on their phone. They are always on their phone more than 5 hours a day no stop .All they do is text back and forward and just have group Chats on social media. They even do it while driving.',\n",
       " 'discourse_type': 'Lead',\n",
       " 'discourse_type_num': 'Lead 1',\n",
       " 'predictionstring': '1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非格式数据处理？？？？？？？？？？？？？？？？？？？？？？？？？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#下载\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"bert-base-chinese\",\n",
    "    #cache_dir=\"./\" #模型保存路径\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.4893,  0.0480, -0.1033,  ...,  0.6812, -0.5084, -0.3696],\n",
       "         [-0.3242,  0.2772, -0.9865,  ..., -0.3170, -0.5676, -0.6787],\n",
       "         [ 1.4281, -1.4412, -1.1542,  ...,  0.6870, -0.3307, -0.4802],\n",
       "         ...,\n",
       "         [ 0.3103,  0.2867, -0.0655,  ...,  0.8095,  0.4969, -0.2483],\n",
       "         [ 0.1014,  0.0843, -0.3567,  ...,  0.6308,  0.0326, -0.1445],\n",
       "         [-0.0165,  0.0576, -0.1530,  ...,  0.4274, -0.3527, -0.5036]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.9998,  1.0000,  0.9990,  0.9671,  0.9570,  0.9432, -0.3311, -0.9890,\n",
       "          0.9939, -0.9989,  1.0000,  0.9998,  0.4891, -0.9078,  0.9999, -0.9998,\n",
       "         -0.2507,  0.9932,  0.9929, -0.0138,  0.9995, -1.0000, -0.8328, -0.6283,\n",
       "          0.1023,  0.9946,  0.9475, -0.9810, -0.9999,  0.9991,  0.9751,  0.9997,\n",
       "          0.9946, -1.0000, -0.9996,  0.6571, -0.2026,  0.9981, -0.4456, -0.8336,\n",
       "         -0.9469, -0.8985, -0.8123, -0.9984, -0.9937,  0.8664, -1.0000, -1.0000,\n",
       "          0.7500,  0.9999, -0.8859, -1.0000,  0.9449, -0.8030, -0.5920,  0.9973,\n",
       "         -0.9999,  0.9741,  1.0000,  0.9592,  0.9997, -0.9876, -0.5223, -0.9999,\n",
       "          1.0000, -0.9998, -0.9873, -0.0514,  1.0000,  1.0000, -0.9800,  0.9941,\n",
       "          1.0000,  0.1028,  0.9840,  0.9996, -0.9990,  0.6320, -1.0000, -0.1577,\n",
       "          1.0000,  0.9990, -0.9418,  0.8916, -0.9883, -1.0000, -0.9971,  1.0000,\n",
       "          0.0778,  0.9989,  0.9995, -0.9979, -1.0000,  0.9984, -0.9991, -0.9997,\n",
       "         -0.6088,  0.9988,  0.6510, -0.9452, -0.6086,  0.9483, -0.9992, -0.9980,\n",
       "          0.9806,  0.9980,  0.6282, -0.9988,  1.0000,  0.2755, -1.0000, -0.9317,\n",
       "         -1.0000, -0.9897, -0.9942,  0.9999, -0.1308,  0.1667,  0.9998, -0.9987,\n",
       "          0.8708, -0.9997, -0.1494,  0.4746,  0.9990,  0.9999,  0.9993, -0.9982,\n",
       "          0.9996,  1.0000,  0.9885,  0.9903, -0.9987,  0.9944,  0.3316, -0.9864,\n",
       "          0.2665, -0.9235,  1.0000,  0.9832,  0.9880, -0.9969,  0.9999, -0.9993,\n",
       "          1.0000, -1.0000,  0.9996, -1.0000, -0.9995,  0.9977,  0.9879,  1.0000,\n",
       "         -0.7710,  1.0000, -0.9977, -1.0000,  0.9940,  0.6044,  0.9974, -0.9999,\n",
       "          0.8613,  0.4166, -0.4448, -0.7926, -1.0000,  1.0000, -0.9054,  1.0000,\n",
       "          0.9989, -0.9687, -0.9794, -0.9990,  0.9132, -0.9998, -0.9279,  0.9937,\n",
       "          0.2282,  0.9989, -0.7021, -0.9248,  0.9873, -0.0348, -1.0000,  0.9980,\n",
       "         -0.6436,  0.8579,  0.5148,  0.5017,  0.9498,  0.7398, -0.8984,  1.0000,\n",
       "          0.3103,  0.9920,  0.9990,  0.4297, -0.8565, -0.9456, -1.0000, -0.8384,\n",
       "          1.0000, -0.7537, -0.9997,  0.7080, -1.0000,  0.9106, -0.0032,  0.4031,\n",
       "         -0.9993, -0.9999,  0.9999, -0.9821, -0.9992,  0.7008, -0.5639, -0.2576,\n",
       "         -0.9998,  0.7380,  0.9621, -0.5300,  0.9049, -0.8513, -0.9994,  0.9983,\n",
       "         -0.9787,  0.8883,  0.5319,  1.0000,  0.9779, -0.8023, -0.8372,  1.0000,\n",
       "          0.1668, -1.0000,  0.9291, -0.9977,  0.1617,  0.9999, -0.9978,  0.6160,\n",
       "          1.0000,  0.9939,  1.0000, -0.0346, -0.9995, -0.9982,  1.0000,  0.9938,\n",
       "          0.9999, -0.9998, -0.9966,  0.4648, -0.9846, -1.0000, -0.9967, -0.5779,\n",
       "          0.9957,  1.0000, -0.4585, -0.9998, -0.8233, -0.9994,  1.0000, -0.9903,\n",
       "          1.0000,  0.9855, -0.9995, -0.9930, -0.0697, -0.7715, -0.9997,  0.7957,\n",
       "         -1.0000, -0.9964, -0.9999,  0.9054, -0.9996, -1.0000,  0.9837,  0.9999,\n",
       "          0.9413, -1.0000,  0.9999,  0.9986, -0.4400, -0.9999,  0.9630, -1.0000,\n",
       "          1.0000, -0.9983,  0.9285, -0.8299, -0.9927,  0.9416,  0.9997,  0.9998,\n",
       "         -0.9980, -0.0119, -0.9850, -0.9922, -0.5638,  0.8188, -0.0474,  0.8873,\n",
       "         -0.9703, -0.4423,  0.7762, -0.9628, -1.0000,  0.9062,  1.0000, -0.9299,\n",
       "          1.0000,  0.8089,  1.0000,  0.8114, -0.9989,  0.9992,  0.8788, -0.9238,\n",
       "         -0.9932, -0.9877,  0.9252,  0.6873, -0.2186, -0.9999,  1.0000,  0.9925,\n",
       "          0.9882,  0.6606,  0.0428,  0.0542,  0.9652, -0.9986,  0.9977, -0.9997,\n",
       "         -0.9826,  0.9998,  1.0000,  0.9996,  0.7297, -0.9481,  0.9866, -0.9988,\n",
       "          0.9994, -0.9998,  0.9986, -0.9700,  0.6322, -0.8479, -0.9978,  1.0000,\n",
       "          0.9923, -0.8584,  0.9998, -0.9053,  0.9773,  0.9907,  0.9958,  0.9832,\n",
       "          0.9687,  1.0000, -0.9983, -0.9950, -0.9471, -0.9932, -0.9992, -1.0000,\n",
       "          0.1383, -0.9989, -0.9908, -0.2207,  0.8576,  0.9865, -0.8341,  0.6704,\n",
       "          0.3915,  0.5625, -0.8157,  0.6454,  0.9749, -0.9958, -0.9937, -1.0000,\n",
       "         -0.9992,  0.7567,  0.9999, -0.9999,  0.9993, -0.9999, -0.9974,  0.9998,\n",
       "         -0.8730, -0.8943,  0.9998, -1.0000,  0.9843,  1.0000,  1.0000,  0.9993,\n",
       "          0.9999, -0.9397, -0.9998, -0.9996, -1.0000, -1.0000, -1.0000,  0.9093,\n",
       "         -0.2561, -1.0000, -0.8312,  0.9890,  1.0000,  0.9763, -0.9995,  0.2109,\n",
       "         -0.9996, -0.9936,  0.9992, -0.9766, -0.9999,  0.9889, -0.3865,  1.0000,\n",
       "         -0.6658,  0.8709,  0.7250,  0.9134,  0.9869, -1.0000,  0.7714,  1.0000,\n",
       "          0.7804, -1.0000, -0.7269, -0.9380, -1.0000, -0.3418,  0.8705,  0.9999,\n",
       "         -1.0000, -0.8958, -0.9977,  0.9047,  0.9953,  0.9999,  0.9998,  0.9301,\n",
       "          0.8164,  0.9979, -0.1239,  0.9999,  0.4748, -0.9992,  0.9992, -0.7648,\n",
       "          0.4372, -0.9999,  0.9987,  0.9156,  1.0000,  0.9884, -0.2510, -0.9693,\n",
       "         -0.9465,  0.9925,  1.0000, -0.9986, -0.9855, -0.9997, -1.0000, -0.9981,\n",
       "         -0.8792, -0.4149, -0.9934, -0.9991,  0.6267,  0.9474,  1.0000,  1.0000,\n",
       "          0.9996, -0.9035, -0.9456,  0.9930,  0.1251,  0.9891, -0.9843, -1.0000,\n",
       "         -0.9984, -0.9998,  0.9999, -0.0648, -0.7437, -0.9304,  0.2382,  0.8682,\n",
       "         -0.9999, -0.7781, -0.9968,  0.8905,  1.0000, -0.9983,  0.9997, -0.9992,\n",
       "          0.8846,  0.6891,  0.8187,  0.9995, -0.1932,  0.3184, -0.7378,  0.8028,\n",
       "          0.8751,  0.9978, -0.9179,  0.7640,  0.9996, -0.8985,  0.9999,  0.4608,\n",
       "          0.8327,  0.9182,  1.0000,  0.4390,  0.9989,  0.9963,  1.0000,  0.9999,\n",
       "         -0.9953,  0.4207,  0.3868, -0.9539, -0.6277,  0.7886,  1.0000,  0.5087,\n",
       "         -0.9788, -0.9999,  0.9914,  0.9997,  1.0000,  0.5314,  0.9985,  0.4909,\n",
       "          0.7666,  0.9007,  0.8938,  0.5903,  0.5780,  0.9964,  0.9996, -0.9999,\n",
       "         -1.0000, -1.0000,  1.0000,  0.9999, -0.9551, -1.0000,  0.9997, -0.8485,\n",
       "          0.9485,  0.9953,  0.1383, -0.4583,  0.7934, -0.9998,  0.0438,  0.9736,\n",
       "          0.8260,  0.6528,  0.9998, -0.9999, -0.0588,  1.0000, -0.3853,  1.0000,\n",
       "          0.2064, -0.9975,  0.9992, -0.9950, -1.0000, -0.8079,  1.0000,  0.9997,\n",
       "         -0.2915, -0.7028,  0.9999, -0.9998,  0.9999, -0.9999,  0.7320, -0.9996,\n",
       "          0.9999, -0.9871, -0.9988, -0.9328,  0.9728,  0.8029, -0.9259,  1.0000,\n",
       "         -0.1275, -0.8398,  0.4345, -0.9656, -0.9971, -0.9887,  0.4897, -1.0000,\n",
       "          0.8242,  0.8222, -0.9278, -0.9876, -1.0000,  1.0000, -0.8206, -0.9880,\n",
       "          0.9999, -0.9849, -1.0000,  0.9300, -0.9989,  0.0406,  0.9891,  0.8704,\n",
       "          0.1355, -1.0000,  0.5408,  1.0000, -0.9989, -0.8145, -0.8150, -0.9712,\n",
       "          0.9889,  0.9984,  0.7006, -0.8014,  0.9357,  0.9969,  0.9146,  0.3066,\n",
       "          0.5914, -0.9993, -0.9997, -0.9799, -0.9986, -0.9999, -1.0000,  1.0000,\n",
       "          0.9999,  1.0000, -0.5310, -0.8730,  0.9501,  0.9934, -0.9996, -0.0382,\n",
       "          0.5533,  0.9623, -0.6595, -0.9995, -0.3326, -1.0000, -0.6415,  0.3089,\n",
       "         -0.8952,  0.6907,  1.0000,  1.0000, -0.9998, -0.9990, -0.9983, -0.9994,\n",
       "          1.0000,  0.9990,  0.9998, -0.9059, -0.8397,  0.9978, -0.7802,  0.4919,\n",
       "         -0.9992, -0.9955, -1.0000,  0.9064, -0.9973, -0.9999,  0.9982,  1.0000,\n",
       "          0.7983, -1.0000, -0.9160,  1.0000,  0.9990,  1.0000,  0.4275,  0.9999,\n",
       "         -0.9941,  0.9957, -0.9990,  1.0000, -1.0000,  1.0000,  0.9999,  0.9993,\n",
       "          0.9982, -0.9929,  0.8084, -0.9424, -0.5462,  0.9672, -0.5580, -0.9951,\n",
       "         -0.0851,  0.9927, -0.8584,  1.0000,  0.7830,  0.4812,  0.6529,  0.5518,\n",
       "          0.9984, -0.9850, -0.9998,  0.9950,  0.9966,  0.9854,  1.0000,  0.9903,\n",
       "          1.0000, -0.9831, -0.9991,  0.9917, -0.8757,  0.3639, -1.0000,  1.0000,\n",
       "          1.0000, -0.9999, -0.9568,  0.3484,  0.6916,  1.0000,  0.9994,  0.9989,\n",
       "          0.8156,  0.5523,  0.9995, -0.9994,  0.9849, -0.9100, -0.9895,  1.0000,\n",
       "         -0.9575,  0.9996, -0.9856,  0.9999, -0.9996,  0.9028,  0.9941,  0.9577,\n",
       "         -0.9948,  1.0000,  0.7443, -0.9983, -0.9982, -0.9972, -0.9985,  0.9254]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model(**tokenizer(\"弱小的我也有大梦想\", return_tensors=\"pt\")) #pt:pytorch,tf:tenserflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'bert-base-cased', \n",
    "        num_labels=3\n",
    ") #文本分类型，各种模型就是for后面的内容不同"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lvwerra/test',\n",
       " 'precision',\n",
       " 'code_eval',\n",
       " 'roc_auc',\n",
       " 'cuad',\n",
       " 'xnli',\n",
       " 'rouge',\n",
       " 'pearsonr',\n",
       " 'mse',\n",
       " 'super_glue',\n",
       " 'comet',\n",
       " 'cer',\n",
       " 'sacrebleu',\n",
       " 'mahalanobis',\n",
       " 'wer',\n",
       " 'competition_math',\n",
       " 'f1',\n",
       " 'recall',\n",
       " 'coval',\n",
       " 'mauve',\n",
       " 'xtreme_s',\n",
       " 'bleurt',\n",
       " 'ter',\n",
       " 'accuracy',\n",
       " 'exact_match',\n",
       " 'indic_glue',\n",
       " 'spearmanr',\n",
       " 'mae',\n",
       " 'squad',\n",
       " 'chrf',\n",
       " 'glue',\n",
       " 'perplexity',\n",
       " 'mean_iou',\n",
       " 'squad_v2',\n",
       " 'meteor',\n",
       " 'bleu',\n",
       " 'wiki_split',\n",
       " 'sari',\n",
       " 'frugalscore',\n",
       " 'google_bleu',\n",
       " 'bertscore',\n",
       " 'matthews_correlation',\n",
       " 'seqeval',\n",
       " 'trec_eval',\n",
       " 'rl_reliability',\n",
       " 'jordyvl/ece',\n",
       " 'angelina-wang/directional_bias_amplification',\n",
       " 'cpllab/syntaxgym',\n",
       " 'lvwerra/bary_score',\n",
       " 'kaggle/amex',\n",
       " 'kaggle/ai4code',\n",
       " 'hack/test_metric',\n",
       " 'yzha/ctc_eval',\n",
       " 'codeparrot/apps_metric',\n",
       " 'mfumanelli/geometric_mean',\n",
       " 'daiyizheng/valid',\n",
       " 'poseval',\n",
       " 'erntkn/dice_coefficient',\n",
       " 'mgfrantz/roc_auc_macro',\n",
       " 'Vlasta/pr_auc',\n",
       " 'gorkaartola/metric_for_tp_fp_samples',\n",
       " 'idsedykh/metric',\n",
       " 'idsedykh/codebleu2',\n",
       " 'idsedykh/codebleu',\n",
       " 'idsedykh/megaglue',\n",
       " 'kasmith/woodscore',\n",
       " 'cakiki/ndcg',\n",
       " 'brier_score',\n",
       " 'Vertaix/vendiscore',\n",
       " 'GMFTBY/dailydialogevaluate',\n",
       " 'GMFTBY/dailydialog_evaluate',\n",
       " 'jzm-mailchimp/joshs_second_test_metric',\n",
       " 'ola13/precision_at_k',\n",
       " 'yulong-me/yl_metric',\n",
       " 'abidlabs/mean_iou',\n",
       " 'abidlabs/mean_iou2',\n",
       " 'KevinSpaghetti/accuracyk',\n",
       " 'Felipehonorato/my_metric',\n",
       " 'NimaBoscarino/weat',\n",
       " 'ronaldahmed/nwentfaithfulness',\n",
       " 'Viona/infolm',\n",
       " 'kyokote/my_metric2',\n",
       " 'kashif/mape',\n",
       " 'Ochiroo/rouge_mn',\n",
       " 'giulio98/code_eval_outputs',\n",
       " 'leslyarun/fbeta_score',\n",
       " 'giulio98/codebleu',\n",
       " 'anz2/iliauniiccocrevaluation',\n",
       " 'zbeloki/m2',\n",
       " 'xu1998hz/sescore',\n",
       " 'mase',\n",
       " 'mape',\n",
       " 'smape',\n",
       " 'dvitel/codebleu',\n",
       " 'NCSOFT/harim_plus',\n",
       " 'JP-SystemsX/nDCG',\n",
       " 'sportlosos/sescore',\n",
       " 'Drunper/metrica_tesi',\n",
       " 'jpxkqx/peak_signal_to_noise_ratio',\n",
       " 'jpxkqx/signal_to_reconstrution_error',\n",
       " 'hpi-dhc/FairEval',\n",
       " 'nist_mt',\n",
       " 'lvwerra/accuracy_score',\n",
       " 'character',\n",
       " 'charcut_mt',\n",
       " 'fengyuli2002/clip_score',\n",
       " 'ybelkada/cocoevaluate']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "evaluate.list_evaluation_modules(\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa5c48cc3434bda9c9c0689be6ec1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        Accuracy\n",
      "\u001b[1;31mString form:\u001b[0m\n",
      "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype=' <...> .4])\n",
      "           >>> print(results)\n",
      "           {'accuracy': 0.8778625954198473}\n",
      "           \"\"\", stored examples: 0)\n",
      "\u001b[1;31mLength:\u001b[0m      0\n",
      "\u001b[1;31mFile:\u001b[0m        c:\\users\\zjt\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14\\accuracy.py\n",
      "\u001b[1;31mDocstring:\u001b[0m  \n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n",
      "Args:\n",
      "    predictions (`list` of `int`): Predicted labels.\n",
      "    references (`list` of `int`): Ground truth labels.\n",
      "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
      "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
      "\n",
      "Returns:\n",
      "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
      "\n",
      "Examples:\n",
      "\n",
      "    Example 1-A simple example\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.5}\n",
      "\n",
      "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
      "        >>> print(results)\n",
      "        {'accuracy': 3.0}\n",
      "\n",
      "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.8778625954198473}\n"
     ]
    }
   ],
   "source": [
    "?accuracy_metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "# 创建TrainingArguments\n",
    "args = TrainingArguments(\n",
    "        'outputs', \n",
    "        learning_rate=8e-5, \n",
    "        warmup_ratio=0.1, \n",
    "        lr_scheduler_type='cosine', \n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\", \n",
    "        per_device_train_batch_size=64, \n",
    "        per_device_eval_batch_size=128,\n",
    "        num_train_epochs=10, \n",
    "        weight_decay=0.01, \n",
    "        report_to='none'\n",
    "        )\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'bert-base-cased', \n",
    "        num_labels=3\n",
    ")\n",
    "# 创建Trainer\n",
    "trainer = Trainer(\n",
    "        model, \n",
    "        args, \n",
    "        train_dataset=[], \n",
    "        eval_dataset=[],\n",
    "        tokenizer=AutoTokenizer.from_pretrained(\"bert-base-chinese\"), \n",
    "        compute_metrics=lambda prey,y:nn.BCEWithLogitsLoss()(prey,y)\n",
    "        )\n",
    "# 模型训练\n",
    "trainer.train()\n",
    "# 模型评估\n",
    "trainer.evaluate()\n",
    "# 模型预测\n",
    "trainer.predict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本预处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"E:/DATA/feedback-prize-effectiveness/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9704a709b505</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>On my perspective, I think that the face is a ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c22adee811b6</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>I think that the face is a natural landform be...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a10d361e54e4</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>If life was on Mars, we would know by now. The...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db3e453ec4e2</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>People thought that the face was formed by ali...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id      essay_id  \\\n",
       "0  0013cc385424  007ACE74B050   \n",
       "1  9704a709b505  007ACE74B050   \n",
       "2  c22adee811b6  007ACE74B050   \n",
       "3  a10d361e54e4  007ACE74B050   \n",
       "4  db3e453ec4e2  007ACE74B050   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Hi, i'm Isaac, i'm going to be writing about h...           Lead   \n",
       "1  On my perspective, I think that the face is a ...       Position   \n",
       "2  I think that the face is a natural landform be...          Claim   \n",
       "3  If life was on Mars, we would know by now. The...       Evidence   \n",
       "4  People thought that the face was formed by ali...   Counterclaim   \n",
       "\n",
       "  discourse_effectiveness  \n",
       "0                Adequate  \n",
       "1                Adequate  \n",
       "2                Adequate  \n",
       "3                Adequate  \n",
       "4                Adequate  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(base_dir+\"/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_type             [Lead, Position, Claim, Evidence, Counterclaim...\n",
       "discourse_effectiveness                   [Adequate, Ineffective, Effective]\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['discourse_type','discourse_effectiveness']].apply(pd.unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4191\n",
      "4191\n"
     ]
    }
   ],
   "source": [
    "print(len(pd.unique(df['essay_id'])))\n",
    "for _,_,files in os.walk(base_dir+\"/train\"):\n",
    "    print(len(files))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8790, 117, 178, 112, 182, 7026, 117, 178, 112, 182, 1280, 1106, 1129, 2269, 1164, 1293, 1142, 1339, 1113, 7403, 1110, 170, 2379, 1657, 13199, 1137, 1191, 1175, 1110, 1297, 1113, 7403, 1115, 1189, 1122, 119, 1109, 1642, 1110, 1164, 1293, 9085, 1261, 170, 3439, 1104, 7403, 1105, 170, 1339, 1108, 1562, 1113, 1103, 5015, 119, 9085, 2144, 112, 189, 1221, 1191, 1103, 1657, 13199, 1108, 1687, 1118, 1297, 1113, 7403, 117, 1137, 1191, 1122, 1110, 1198, 170, 2379, 1657, 13199, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = \"E:/DATA/feedback-prize-effectiveness/\"\n",
    "df = pd.read_csv(base_dir+\"/train.csv\")\n",
    "\n",
    "needed_col = ['essay_id','discourse_text','discourse_type','discourse_effectiveness']\n",
    "df = df[needed_col]\n",
    "df.columns=['id','text','type','ef']\n",
    "\n",
    "typ = {'Lead':1,'Position':2, 'Claim':3, 'Evidence':4, 'Counterclaim':5, 'Rebuttal':6, 'Concluding Statement':7}\n",
    "eff = {'Adequate':1, 'Ineffective':2, 'Effective':3}\n",
    "df['type'] = df['type'].apply(lambda x:typ[x])\n",
    "df['ef'] = df['ef'].apply(lambda x:eff[x])\n",
    "\n",
    "#text处理\n",
    "tk = AutoTokenizer.from_pretrained('bert-base-cased',use_fast=True)\n",
    "tk.max_len=384\n",
    "df['text'] = df['text'].apply(lambda x:tk(x,truncation=True))\n",
    "\n",
    "df.iloc[0,1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "modules_dir已存在，已将其清空\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "base_dir = \"E:/DATA/feedback-prize-effectiveness/\"\n",
    "\n",
    "input_len = 384\n",
    "train_batch_size=64\n",
    "test_batch_size=128\n",
    "epochs=15\n",
    "\n",
    "seed=101\n",
    "def set_seed():\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mode='debug'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = AutoTokenizer.from_pretrained('bert-base-cased',use_fast=True)\n",
    "tk.max_len=input_len\n",
    "\n",
    "class DfProc():\n",
    "    def __init__(self, data_mode='debug'):\n",
    "\n",
    "        self.data_mode = data_mode\n",
    "        assert self.data_mode in ['debug','mini','all'],print(\"mode值错误\")\n",
    "\n",
    "        self.__Set_Df()\n",
    "\n",
    "\n",
    "    def __Set_Df(self):\n",
    "\n",
    "        #train\n",
    "        df = pd.read_csv(base_dir+\"/train.csv\")\n",
    "\n",
    "        #这里的策略是将type列作为输入文本的开头，则type列不再需要\n",
    "        df['discourse_text'] = df['discourse_type']+ tk.sep_token + df['discourse_text'] #特殊符号\n",
    "\n",
    "        #skf = StratifiedGroupKFold(5)\n",
    "        #for i, (train_i,valid_i) in enumerate(skf.split(df,df['discourse_type'],groups=df['essay_id'])):\n",
    "        #    df.loc[valid_i,'fold'] = i+1\n",
    "\n",
    "\n",
    "        needed_col = ['essay_id','discourse_text','discourse_effectiveness']\n",
    "        df = df[needed_col]\n",
    "        df.columns=['id','text','ef']\n",
    "\n",
    "        eff = {'Adequate':1, 'Ineffective':2, 'Effective':3}\n",
    "        df['ef'] = df['ef'].apply(lambda x:eff[x])\n",
    "\n",
    "        #text处理\n",
    "        df['text'] = df['text'].apply(lambda x:tk(x,truncation=True))\n",
    "\n",
    "        #train_df = df[df['fold']!=1]\n",
    "        #valid_df = df[df['fold']==1]\n",
    "        train_df = df\n",
    "\n",
    "\n",
    "        if self.data_mode == 'all':\n",
    "            self.train_df = train_df\n",
    "            #self.valid_df = valid_df\n",
    "        elif self.data_mode == 'debug':\n",
    "            self.train_df = train_df[:2*train_batch_size]\n",
    "            #self.valid_df = valid_df[:2*test_batch_size]\n",
    "        else:\n",
    "            self.train_df = train_df[:int(0.3*len(train_df))]\n",
    "            #self.valid_df = valid_df[:int(0.3*len(valid_df))]\n",
    "\n",
    "        #test\n",
    "        df = pd.read_csv(base_dir+\"/test.csv\")\n",
    "\n",
    "        df['discourse_text'] = df['discourse_type']+ tk.sep_token + df['discourse_text']\n",
    "\n",
    "        needed_col = ['essay_id', 'discourse_text']\n",
    "        df = df[needed_col]\n",
    "        df.columns=['id','text']\n",
    "\n",
    "        df['text'] = df['text'].apply(lambda x:tk(x,truncation=True))\n",
    "\n",
    "        if self.data_mode == 'all':\n",
    "            self.test_df = df\n",
    "        elif self.data_mode == 'debug':\n",
    "            self.test_df = df[:2*test_batch_size]\n",
    "        else:\n",
    "            self.test_df = df[:int(0.3*len(df))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d233a11d4546c6b43e78d97b88728d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\project\\python\\feedback-prize-effectiveness\\exp.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mbert-base-cased\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         num_labels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         model, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         args, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         compute_metrics\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m prey,y:nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()(prey,y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m trainer \u001b[39m=\u001b[39m get_trainer(DfProc())\n",
      "\u001b[1;32me:\\project\\python\\feedback-prize-effectiveness\\exp.ipynb Cell 31\u001b[0m in \u001b[0;36mget_trainer\u001b[1;34m(dfProc)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_trainer\u001b[39m(dfProc):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         learning_rate\u001b[39m=\u001b[39m\u001b[39m8e-5\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         report_to\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mbert-base-cased\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         num_labels\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         model, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         args, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         compute_metrics\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m prey,y:nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()(prey,y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    462\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    465\u001b[0m     )\n\u001b[0;32m    466\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py:2137\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2123\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2124\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m   2125\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2126\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2135\u001b[0m         _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   2136\u001b[0m     )\n\u001b[1;32m-> 2137\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   2139\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2140\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m SAFE_WEIGHTS_NAME:\n\u001b[0;32m   2142\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[0;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[0;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[0;32m    122\u001b[0m     )\n\u001b[1;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:1242\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[0;32m   1240\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[1;32m-> 1242\u001b[0m     http_get(\n\u001b[0;32m   1243\u001b[0m         url_to_download,\n\u001b[0;32m   1244\u001b[0m         temp_file,\n\u001b[0;32m   1245\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1246\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1247\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1248\u001b[0m     )\n\u001b[0;32m   1250\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, blob_path)\n\u001b[0;32m   1251\u001b[0m _chmod_and_replace(temp_file\u001b[39m.\u001b[39mname, blob_path)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:495\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[0;32m    486\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    487\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    488\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    489\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    494\u001b[0m )\n\u001b[1;32m--> 495\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[0;32m    496\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    497\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py:627\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 627\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    629\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    630\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py:566\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    563\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    565\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 566\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py:532\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    530\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_trainer(dfProc):\n",
    "    args = TrainingArguments(\n",
    "        'outputs', \n",
    "        learning_rate=8e-5, \n",
    "        warmup_ratio=0.1, \n",
    "        lr_scheduler_type='cosine', \n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\", \n",
    "        per_device_train_batch_size=train_batch_size, \n",
    "        per_device_eval_batch_size=test_batch_size,\n",
    "        num_train_epochs=epochs, \n",
    "        weight_decay=0.01, \n",
    "        report_to='none'\n",
    "        )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'bert-base-cased', \n",
    "        num_labels=3\n",
    "        )\n",
    "    return Trainer(\n",
    "        model, \n",
    "        args, \n",
    "        train_dataset=dfProc.train_df, \n",
    "        eval_dataset=dfProc.test_df,\n",
    "        tokenizer=tk, \n",
    "        compute_metrics=lambda prey,y:nn.BCEWithLogitsLoss()(prey,y)\n",
    "        )\n",
    "trainer = get_trainer(DfProc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
