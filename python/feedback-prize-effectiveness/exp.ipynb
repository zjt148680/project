{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"E:/DATA/feedback-prize-effectiveness/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   discourse_id      essay_id  \\\n",
      "0  0013cc385424  007ACE74B050   \n",
      "1  9704a709b505  007ACE74B050   \n",
      "2  c22adee811b6  007ACE74B050   \n",
      "3  a10d361e54e4  007ACE74B050   \n",
      "4  db3e453ec4e2  007ACE74B050   \n",
      "5  36a565e45db7  007ACE74B050   \n",
      "6  fb65fe816ba3  007ACE74B050   \n",
      "7  4e472e2584fa  007ACE74B050   \n",
      "8  28a94d3ee425  007ACE74B050   \n",
      "9  d226f06362f5  00944C693682   \n",
      "\n",
      "                                      discourse_text        discourse_type  \\\n",
      "0  Hi, i'm Isaac, i'm going to be writing about h...                  Lead   \n",
      "1  On my perspective, I think that the face is a ...              Position   \n",
      "2  I think that the face is a natural landform be...                 Claim   \n",
      "3  If life was on Mars, we would know by now. The...              Evidence   \n",
      "4  People thought that the face was formed by ali...          Counterclaim   \n",
      "5  though some say that life on Mars does exist, ...              Rebuttal   \n",
      "6  It says in paragraph 7, on April 5, 1998, Mars...              Evidence   \n",
      "7  Everyone who thought it was made by alieans ev...          Counterclaim   \n",
      "8  Though people were not satified about how the ...  Concluding Statement   \n",
      "9  Limiting the usage of cars has personal and pr...                  Lead   \n",
      "\n",
      "  discourse_effectiveness  \n",
      "0                Adequate  \n",
      "1                Adequate  \n",
      "2                Adequate  \n",
      "3                Adequate  \n",
      "4                Adequate  \n",
      "5             Ineffective  \n",
      "6                Adequate  \n",
      "7                Adequate  \n",
      "8                Adequate  \n",
      "9               Effective  \n",
      "Hi, i'm Isaac, i'm going to be writing about how this face on Mars is a natural landform or if there is life on Mars that made it. The story is about how NASA took a picture of Mars and a face was seen on the planet. NASA doesn't know if the landform was created by life on Mars, or if it is just a natural landform. \n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(data_dir+\"train.csv\")\n",
    "print(df1.head(10))\n",
    "print(df1.iloc[0]['discourse_text'])\n",
    "#essay_id   txt文件名\n",
    "#discourse_id   段落的id，应该没用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   discourse_id      essay_id  \\\n",
      "0  a261b6e14276  D72CB1C11673   \n",
      "1  5a88900e7dc1  D72CB1C11673   \n",
      "2  9790d835736b  D72CB1C11673   \n",
      "3  75ce6d68b67b  D72CB1C11673   \n",
      "4  93578d946723  D72CB1C11673   \n",
      "\n",
      "                                      discourse_text discourse_type  \n",
      "0  Making choices in life can be very difficult. ...           Lead  \n",
      "1  Seeking multiple opinions can help a person ma...       Position  \n",
      "2                     it can decrease stress levels           Claim  \n",
      "3             a great chance to learn something new           Claim  \n",
      "4               can be very helpful and beneficial.           Claim  \n",
      "Making choices in life can be very difficult. People often ask for advice when they can not decide on one thing. It's always good to ask others for their advice when making a choice. When you have multiple opinions you have the ability to make the best choice for yourself. \n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(data_dir+\"test.csv\")\n",
    "print(df2.head())\n",
    "print(df2.iloc[0]['discourse_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear: Principal\n",
      "\n",
      "I am arguing against the policy change because even though there are some children out there that really needs help with their academic work, that does not mean that only because they have a c average that would not let them enjoy their sports or other activities unless they've a B average.\n",
      "\n",
      "Sometimes teachers or even principal needs to consider that we should give the help that any student should have. Also this may consider student self as steam. Meaning student would start to feel sad nervous, and not wanting to go to school because of the reason they have a low averages and they can not participate in other activities or sports. The fact that there are children that would want to enjoy many good things the school is actually giving it to them.\n",
      "\n",
      "We would want to make changes as, \"like to be a better person for a better tomorrow\" This supports the idea of having have many good thoughts and incasing your work as much as possible. In some situation like arguing we should make a vote to see if kids would want to have a school policy of change and having to participate in fun activities but they first need to have at least a B average. Some reasons I would be against the school policy change is because you would feel ashame and then many bad things could happen meaning, you would be angry etc. In additionally , I think that student should have sports because it helps them with their health, and problems that they would have personal. I am against this policy change.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "Student.\n",
      "-------\n",
      "Dear Florida senator,\n",
      "\n",
      "I would like disuss why changing up the way we voyte today is not as bad f an idea as it sounds. Changing to elections by a popular vote sounds like its in the peoples hands instead of juts casting out votes on the slate electors. Aren't we voting for our president. innstead of countting onn the electoral votes to hep out or, go against out favor, it'd be much more easier to just let the people vote.\n",
      "\n",
      "The elctoral votes are utterly useless or unimportant,though. If there is ever happens to be a tie between the two candidates the electoral votes can help break it. It'd be less of a hassle to have the florida residents vote,directly, on someone besides the president. Less worring about if theyre going to win or not. Now when you start to think about it the electoral college is just unfair an pretty outdated,if you ask me. Change doesn't sound that bad. All im trying to achive by writting this, is to gain actual control over who we're all voting for.\n",
      "\n",
      "It's also not just what's completely wrong with the electoral college but how does it help in the first place? When you start analizing it, it really doens't help much. How much actual help can the lectora coellge make, in my defense not much. Why? Simple, its just 20-30 more votes and not many states have that much Hawaii has uop to 4 electoral votes, not much. Voters should be alowed to directly vote for who they want to be president and not rely on the lectoral college.    \n"
     ]
    }
   ],
   "source": [
    "with open(data_dir+\"/train/000E6DE9E817.txt\") as f:\n",
    "    print(f.read())\n",
    "print(\"-------\")\n",
    "with open(data_dir+\"/train/00B144412785.txt\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn\n",
    "\n",
    "## 数据基本处理\n",
    "1. 一般语言处理中对全部数据只会取常用的n个词，在此之外的词是不认识的，即先有一本字典\n",
    "2. 用one-hot编码时，每个词都表示为长n的向量，其中只有一个值是1，其余全是0。比如字典的第i（0开始）个单词其编码中1值的下标是i\n",
    "\n",
    "## embedding\n",
    "1. one_hot编码稀疏，所以考虑用稠密向量表示词，比如只用长为m<n的向量表示n个单词，其中单词间关系可以体现在向量间关系中，比如$\\vec{男}+\\vec{国王}=\\vec{皇帝}$\n",
    "2. 可由embedding层实现这个，embedding记录了一个(n,m)的矩阵，每行都是一个单词的稠密向量，作用是one-hot编码的向量按照其1值的下标i访问这个矩阵第i行，取出这行向量作为新输入\n",
    "3. 具体使用看下面代码说明\n",
    "4. embedding可由自行训练出，也可预加载预训练参数。使用预训练参数时，冻结此层\n",
    "\n",
    "## 初步结果\n",
    "将batch_size\\*len_sentences\\*n变为batch_size\\*len_sentences\\*m\n",
    "\n",
    "## rnn具体流程\n",
    "1. 首先初始化hadden_input为全0\n",
    "2. 对每个词，其都会和当前的hadden_input一起进入网络(cat或add)进行一步（liner、tanh激活）运算，所得的输出作为新的hadden_input与下一个词的向量一起进入网络（cat或add）\n",
    "3. cat（最后一维）的话，需要截断，或者另外卷积一次获得新hadden_input\n",
    "4. pytorch的rnn为x[i]通过一liner，hidden通过一liner，两个结果相加经Tanh激活，结果作为x[i]和新hidden，具体见下面代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before embedding:torch.Size([4, 20])\n",
      "after embedding:torch.Size([4, 20, 8])\n",
      "-------------\n",
      "rnn input:torch.Size([20, 4, 8])\n",
      "rnn out:torch.Size([20, 4, 128])\n",
      "rnn outh:torch.Size([4, 128])\n",
      "-----------\n",
      "finally shape:torch.Size([4, 2])\n",
      "tensor([[0.5058, 0.4942],\n",
      "        [0.5616, 0.4384],\n",
      "        [0.4322, 0.5678],\n",
      "        [0.4265, 0.5735]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# rnn内部细节\n",
    "batch_size = 4\n",
    "sentence_len = 20 #一句话20个词\n",
    "words_num = 100 #字典记录了100个词\n",
    "words_len = 8 #字典的每个词向量长8\n",
    "X = torch.randint(0,words_num,[batch_size,sentence_len]) #注意输入不为one-hot，只是每个词的字典序号，比如[3,2,10]表示一句话。int\n",
    "y = torch.as_tensor([[0.,1],[0,1],[1,0],[1,0]])\n",
    "#print(X)\n",
    "\n",
    "hidden_layer_num = 1 #多少个hidden用于循环，即多少个循环部分\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = 128\n",
    "        self.embedding_layer = nn.Embedding(words_num,words_len,padding_idx=0) #100*8,padding_idx为输入长度不够时填充的字典词序号\n",
    "\n",
    "        #一个循环节\n",
    "        self.hidden_layer_x = nn.Linear(words_len,self.hidden_size) #rnn关键部分\n",
    "        self.hidden_layer_h = nn.Linear(self.hidden_size,self.hidden_size) #给h用\n",
    "\n",
    "        #分类器\n",
    "        self.out_layer = nn.Linear(self.hidden_size,2)\n",
    "        self.activation_layer = nn.Softmax(dim=-1) #dim=0表示a[i][j][k]按i方向的几个数一起算\n",
    "\n",
    "    def __init_hidden__(self):\n",
    "        return torch.zeros([hidden_layer_num,batch_size,self.hidden_size])\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.hidden = self.__init_hidden__()\n",
    "\n",
    "        print(f\"before embedding:{x.shape}\") #[4, 20]\n",
    "        x = self.embedding_layer(x)\n",
    "        print(f\"after embedding:{x.shape}\") #[4, 20, 8]\n",
    "        print(\"-------------\")\n",
    "\n",
    "        out = torch.zeros([sentence_len, batch_size, self.hidden_size])\n",
    "\n",
    "        #rnn部分\n",
    "        # 为了更好计算，将数据x变形为为len_sencentces*batch_size*words_num\n",
    "        # 即x[0]为各句子首单词\n",
    "        x = x.transpose(0,1)\n",
    "        print(f\"rnn input:{x.shape}\") #[20, 4, 128]\n",
    "        for i in range(x.shape[0]):\n",
    "            a1 = self.hidden_layer_x(x[i])\n",
    "            a2 = self.hidden_layer_h(self.hidden[0])\n",
    "\n",
    "            out[i] = self.hidden[0] = nn.Tanh()(a1+a2)\n",
    "        print(f\"rnn out:{out.shape}\")\n",
    "        print(f\"rnn outh:{self.hidden[0].shape}\")\n",
    "        print(\"-----------\")\n",
    "        \n",
    "        #分类器\n",
    "        o = self.hidden[0]\n",
    "        o = self.out_layer(o)\n",
    "        o = self.activation_layer(o)\n",
    "        print(f\"finally shape:{o.shape}\") #[4, 2]\n",
    "        print(o)\n",
    "\n",
    "        return o\n",
    "\n",
    "rnn = MyRNN()\n",
    "out = rnn(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch的rnn\n",
    "- 首先就是输入是batch_size\\*len_sencentces\\*words_num  \n",
    "所以Embedding算是预处理部分，如果需要训练则？？？？？？？？\n",
    "- 主要公式$$h_t = \\tanh(x_t W_{ih}^T + b_{ih} + h_{t-1}W_{hh}^T + b_{hh})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 8])\n",
      "torch.Size([2, 5, 32]) torch.Size([1, 2, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "sentence_len = 5\n",
    "words_num = 10\n",
    "words_len = 8\n",
    "\n",
    "X = torch.randint(0,words_num,[batch_size,sentence_len])\n",
    "\n",
    "X = nn.Embedding(words_num,words_len)(X)\n",
    "#X = X.transpose(0,1) #batch_size放在第二维，则batch_first设置为False\n",
    "print(X.shape)\n",
    "\n",
    "hidden_size=32\n",
    "num_layers=1\n",
    "H = torch.zeros([num_layers,batch_size,hidden_size]) #可以不设置，则默认为0，这是单向rnn\n",
    "#HH = torch.zeros([num_layers*2,batch_size,hidden_size]) #双向rnn，需要RNN中设置bidirectional=True\n",
    "\n",
    "rnn = nn.RNN(\n",
    "            input_size=words_len,\n",
    "\n",
    "            #hidden_size，num_layers 都是对网络的设置，与输入数据无关，设置相对自由\n",
    "            hidden_size=hidden_size,     # hidden层大小\n",
    "            num_layers=num_layers,       # n个rnn层\n",
    "\n",
    "            batch_first=True, #True则输入输出的batch在第一维，否则在第二维（参照上面MyRNN在hidden前的变形）\n",
    "\n",
    "            bidirectional=False, #是否双向rnn\n",
    "        )\n",
    "\n",
    "out,outh = rnn(X,H)\n",
    "print(out.shape,outh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([32, 8]) <class 'torch.nn.parameter.Parameter'>\n",
      "weight_hh_l0 torch.Size([32, 32]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias_ih_l0 torch.Size([32]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias_hh_l0 torch.Size([32]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for i in rnn.named_parameters():\n",
    "    print(i[0],i[1].shape,type(i[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 4])\n",
      "pytorch部分\n",
      "torch.Size([2, 5, 8]) tensor([ 0.0445,  0.4097,  0.3055, -0.1414, -0.7245,  0.5351, -0.2224, -0.1610],\n",
      "       grad_fn=<SliceBackward0>) torch.Size([1, 2, 8]) tensor([-0.0913,  0.1614, -0.0179, -0.8154,  0.3454, -0.7803, -0.3056,  0.3397],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "-------------------\n",
      "MyCNN部分\n",
      "torch.Size([2, 5, 8]) tensor([ 0.0445,  0.4097,  0.3055, -0.1414, -0.7245,  0.5351, -0.2224, -0.1610],\n",
      "       grad_fn=<SliceBackward0>) torch.Size([1, 2, 8]) tensor([-0.0913,  0.1614, -0.0179, -0.8154,  0.3454, -0.7803, -0.3056,  0.3397],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(43)\n",
    "\n",
    "batch_size = 2\n",
    "sentence_len = 5\n",
    "words_num = 10\n",
    "words_len = 4\n",
    "\n",
    "hidden_size=8\n",
    "num_layers=1\n",
    "\n",
    "X = torch.randint(0,words_num,(batch_size,sentence_len))\n",
    "em = nn.Embedding(words_num,words_len)\n",
    "X = em(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('pytorch部分')\n",
    "rnn = nn.RNN(\n",
    "            input_size=words_len,\n",
    "\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "\n",
    "            batch_first=True,\n",
    "        )\n",
    "out,outh = rnn(X)\n",
    "print(out.shape,out[0,0,:],outh.shape,outh[0,0,:])\n",
    "print('-------------------')\n",
    "\n",
    "\n",
    "print('MyCNN部分')\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.hidden_layer_x = nn.Linear(words_len,hidden_size) #rnn关键部分\n",
    "        self.hidden_layer_h = nn.Linear(hidden_size,hidden_size) #h用\n",
    "\n",
    "        params = list(rnn.parameters())\n",
    "\n",
    "        self.hidden_layer_x.weight = params[0]\n",
    "        self.hidden_layer_h.weight = params[1]\n",
    "        self.hidden_layer_x.bias = params[2]\n",
    "        self.hidden_layer_h.bias = params[3]\n",
    "\n",
    "    def __init_hidden__(self):\n",
    "        return torch.zeros([num_layers,batch_size,hidden_size])\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.hidden = self.__init_hidden__()\n",
    "\n",
    "        out = torch.zeros([sentence_len,batch_size,hidden_size])\n",
    "\n",
    "        x = x.transpose(0,1)\n",
    "        for i in range(x.shape[0]):\n",
    "            a1 = self.hidden_layer_x(x[i])\n",
    "            a2 = self.hidden_layer_h(self.hidden[0])\n",
    "            self.hidden[0] = nn.Tanh()(a1+a2)\n",
    "\n",
    "\n",
    "            out[i] = self.hidden[0]\n",
    "\n",
    "        return out.transpose(0,1)\n",
    "\n",
    "myrnn = MyRNN()\n",
    "out = myrnn(X)\n",
    "print(out.shape,out[0,0,:],myrnn.hidden.shape,myrnn.hidden[0,0,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "- rnn因为激活层是Tanh，显然，当前信息几乎不会对很远的计算产生影响，即只能短期记忆\n",
    "- lstm中引入可以选择“记忆”和“当前”的信息对当前输出的占比  \n",
    "  \n",
    "nn.LSTM为什么有两个参数矩阵？有一个是4个矩阵组合，这四个矩阵是给四个部分用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 4])\n",
      "pytorch部分\n",
      "torch.Size([2, 5, 4]) tensor([-0.2213,  0.1023, -0.2557, -0.2017], grad_fn=<SliceBackward0>) torch.Size([1, 2, 4]) torch.Size([1, 2, 4])\n",
      "-------------------\n",
      "MyLSTM部分\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MyLSTM' object has no attribute 'hidden_layer_xh1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\project\\python\\feedback-prize-effectiveness\\exp.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X16sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X16sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X16sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m mylstm \u001b[39m=\u001b[39m MyLSTM()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X16sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m out \u001b[39m=\u001b[39m mylstm(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X16sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39mshape,out[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,:],mylstm\u001b[39m.\u001b[39mC\u001b[39m.\u001b[39mshape,mylstm\u001b[39m.\u001b[39mH\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;32me:\\project\\python\\feedback-prize-effectiveness\\exp.ipynb Cell 14\u001b[0m in \u001b[0;36mMyLSTM.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_layer_xh \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(hidden_size\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m,hidden_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(lstm\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_layer_xh1\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m params[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_layer_xh2\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m params[\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_layer_xh2\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m params[\u001b[39m2\u001b[39m]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MyLSTM' object has no attribute 'hidden_layer_xh1'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(43)\n",
    "\n",
    "batch_size = 2\n",
    "sentence_len = 5\n",
    "words_num = 10\n",
    "words_len = 4\n",
    "\n",
    "hidden_size=4\n",
    "num_layers=1\n",
    "\n",
    "X = torch.randint(0,words_num,(batch_size,sentence_len))\n",
    "em = nn.Embedding(words_num,words_len)\n",
    "X = em(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('pytorch部分')\n",
    "lstm = nn.LSTM(\n",
    "            input_size=words_len,\n",
    "\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "\n",
    "            batch_first=True,\n",
    "        )\n",
    "H = torch.zeros([num_layers,batch_size,hidden_size])\n",
    "C = torch.zeros([num_layers,batch_size,hidden_size])\n",
    "out,(outh,outc) = lstm(X,(H,C)) #H、C默认为0\n",
    "print(out.shape,out[0,0,:],outh.shape,outc.shape)\n",
    "print('-------------------')\n",
    "\n",
    "\n",
    "print('MyLSTM部分')\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.\n",
    "\n",
    "        self.hidden_layer_xh = nn.Linear(hidden_size*4,hidden_size)\n",
    "\n",
    "        params = list(lstm.parameters())\n",
    "        self.hidden_layer_xh1.weight = params[0]\n",
    "        self.hidden_layer_xh2.weight = params[1]\n",
    "        self.hidden_layer_xh2.bias = params[2]\n",
    "        self.hidden_layer_xh2.bias = params[3]\n",
    "\n",
    "    def __init_H_C__(self):\n",
    "        return torch.zeros([1,batch_size,hidden_size]),torch.zeros([1,batch_size,hidden_size])\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.H,self.C = self.__init_H_C__()\n",
    "        #H是输出\n",
    "        #C是记忆\n",
    "\n",
    "        x = x.transpose(0,1)\n",
    "        for i in torch.arange(x.shape[0]):\n",
    "            hidden1 = self.hidden_layer_xh1(x[i]+self.H)\n",
    "            hidden2 = self.hidden_layer_xh2(x[i]+self.H)\n",
    "\t\t\t\n",
    "            cp = nn.Sigmoid()(hidden1) #C即记忆的保留\n",
    "            self.C = self.C*cp+nn.Tanh()(hidden2)*(1-cp) #更新C\n",
    "            self.H = nn.Tanh()(self.C)*nn.Sigmoid()(hidden1) #生成新的输出\n",
    "\n",
    "        x = x.transpose(0,1)\n",
    "        return x\n",
    "\n",
    "mylstm = MyLSTM()\n",
    "out = mylstm(X)\n",
    "print(out.shape,out[0,0,:],mylstm.C.shape,mylstm.H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for i in lstm.parameters():\n",
    "    print(i.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
