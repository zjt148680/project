{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "from torch.utils import data\n",
    "from torch.cuda import amp\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gc #垃圾回收\n",
    "\n",
    "from tqdm import tqdm #进度条 for data in tqdm(range(100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"E:/DATA/feedback-prize-effectiveness/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   discourse_id      essay_id  \\\n",
      "0  0013cc385424  007ACE74B050   \n",
      "1  9704a709b505  007ACE74B050   \n",
      "2  c22adee811b6  007ACE74B050   \n",
      "3  a10d361e54e4  007ACE74B050   \n",
      "4  db3e453ec4e2  007ACE74B050   \n",
      "5  36a565e45db7  007ACE74B050   \n",
      "6  fb65fe816ba3  007ACE74B050   \n",
      "7  4e472e2584fa  007ACE74B050   \n",
      "8  28a94d3ee425  007ACE74B050   \n",
      "9  d226f06362f5  00944C693682   \n",
      "\n",
      "                                      discourse_text        discourse_type  \\\n",
      "0  Hi, i'm Isaac, i'm going to be writing about h...                  Lead   \n",
      "1  On my perspective, I think that the face is a ...              Position   \n",
      "2  I think that the face is a natural landform be...                 Claim   \n",
      "3  If life was on Mars, we would know by now. The...              Evidence   \n",
      "4  People thought that the face was formed by ali...          Counterclaim   \n",
      "5  though some say that life on Mars does exist, ...              Rebuttal   \n",
      "6  It says in paragraph 7, on April 5, 1998, Mars...              Evidence   \n",
      "7  Everyone who thought it was made by alieans ev...          Counterclaim   \n",
      "8  Though people were not satified about how the ...  Concluding Statement   \n",
      "9  Limiting the usage of cars has personal and pr...                  Lead   \n",
      "\n",
      "  discourse_effectiveness  \n",
      "0                Adequate  \n",
      "1                Adequate  \n",
      "2                Adequate  \n",
      "3                Adequate  \n",
      "4                Adequate  \n",
      "5             Ineffective  \n",
      "6                Adequate  \n",
      "7                Adequate  \n",
      "8                Adequate  \n",
      "9               Effective  \n",
      "Hi, i'm Isaac, i'm going to be writing about how this face on Mars is a natural landform or if there is life on Mars that made it. The story is about how NASA took a picture of Mars and a face was seen on the planet. NASA doesn't know if the landform was created by life on Mars, or if it is just a natural landform. \n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(data_dir+\"train.csv\")\n",
    "print(df1.head(10))\n",
    "print(df1.iloc[0]['discourse_text'])\n",
    "#essay_id   txt文件名\n",
    "#discourse_id   段落的id，应该没用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   discourse_id      essay_id  \\\n",
      "0  a261b6e14276  D72CB1C11673   \n",
      "1  5a88900e7dc1  D72CB1C11673   \n",
      "2  9790d835736b  D72CB1C11673   \n",
      "3  75ce6d68b67b  D72CB1C11673   \n",
      "4  93578d946723  D72CB1C11673   \n",
      "\n",
      "                                      discourse_text discourse_type  \n",
      "0  Making choices in life can be very difficult. ...           Lead  \n",
      "1  Seeking multiple opinions can help a person ma...       Position  \n",
      "2                     it can decrease stress levels           Claim  \n",
      "3             a great chance to learn something new           Claim  \n",
      "4               can be very helpful and beneficial.           Claim  \n",
      "Making choices in life can be very difficult. People often ask for advice when they can not decide on one thing. It's always good to ask others for their advice when making a choice. When you have multiple opinions you have the ability to make the best choice for yourself. \n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(data_dir+\"test.csv\")\n",
    "print(df2.head())\n",
    "print(df2.iloc[0]['discourse_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear: Principal\n",
      "\n",
      "I am arguing against the policy change because even though there are some children out there that really needs help with their academic work, that does not mean that only because they have a c average that would not let them enjoy their sports or other activities unless they've a B average.\n",
      "\n",
      "Sometimes teachers or even principal needs to consider that we should give the help that any student should have. Also this may consider student self as steam. Meaning student would start to feel sad nervous, and not wanting to go to school because of the reason they have a low averages and they can not participate in other activities or sports. The fact that there are children that would want to enjoy many good things the school is actually giving it to them.\n",
      "\n",
      "We would want to make changes as, \"like to be a better person for a better tomorrow\" This supports the idea of having have many good thoughts and incasing your work as much as possible. In some situation like arguing we should make a vote to see if kids would want to have a school policy of change and having to participate in fun activities but they first need to have at least a B average. Some reasons I would be against the school policy change is because you would feel ashame and then many bad things could happen meaning, you would be angry etc. In additionally , I think that student should have sports because it helps them with their health, and problems that they would have personal. I am against this policy change.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "Student.\n",
      "-------\n",
      "Dear Florida senator,\n",
      "\n",
      "I would like disuss why changing up the way we voyte today is not as bad f an idea as it sounds. Changing to elections by a popular vote sounds like its in the peoples hands instead of juts casting out votes on the slate electors. Aren't we voting for our president. innstead of countting onn the electoral votes to hep out or, go against out favor, it'd be much more easier to just let the people vote.\n",
      "\n",
      "The elctoral votes are utterly useless or unimportant,though. If there is ever happens to be a tie between the two candidates the electoral votes can help break it. It'd be less of a hassle to have the florida residents vote,directly, on someone besides the president. Less worring about if theyre going to win or not. Now when you start to think about it the electoral college is just unfair an pretty outdated,if you ask me. Change doesn't sound that bad. All im trying to achive by writting this, is to gain actual control over who we're all voting for.\n",
      "\n",
      "It's also not just what's completely wrong with the electoral college but how does it help in the first place? When you start analizing it, it really doens't help much. How much actual help can the lectora coellge make, in my defense not much. Why? Simple, its just 20-30 more votes and not many states have that much Hawaii has uop to 4 electoral votes, not much. Voters should be alowed to directly vote for who they want to be president and not rely on the lectoral college.    \n"
     ]
    }
   ],
   "source": [
    "with open(data_dir+\"/train/000E6DE9E817.txt\") as f:\n",
    "    print(f.read())\n",
    "print(\"-------\")\n",
    "with open(data_dir+\"/train/00B144412785.txt\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "## 数据基本处理\n",
    "1. 一般语言处理中对全部数据只会取常用的n个词，在此之外的词是不认识的，即先有一本字典\n",
    "2. 用one-hot编码时，每个词都表示为长n的向量，其中只有一个值是1，其余全是0。比如字典的第i（0开始）个单词其编码中1值的下标是i\n",
    "\n",
    "## embedding\n",
    "1. one_hot编码稀疏，所以考虑用稠密向量表示词，比如只用长为m<n的向量表示n个单词，其中单词间关系可以体现在向量间关系中，比如$\\vec{男}+\\vec{国王}=\\vec{皇帝}$\n",
    "2. 可由embedding层实现这个，embedding记录了一个(n,m)的矩阵，每行都是一个单词的稠密向量，作用是one-hot编码的向量按照其1值的下标i访问这个矩阵第i行，取出这行向量作为新输入\n",
    "3. 具体使用看下面代码说明\n",
    "4. embedding可由自行训练出，也可预加载预训练参数。使用预训练参数时，冻结此层\n",
    "\n",
    "## 初步结果\n",
    "将batch_size\\*len_sentences\\*n变为batch_size\\*len_sentences\\*m\n",
    "\n",
    "## rnn具体流程\n",
    "1. 首先初始化hadden_input为全0\n",
    "2. 对每个词，其都会和当前的hadden_input一起进入网络(cat或add)进行一步（liner、tanh激活）运算，所得的输出作为新的hadden_input与下一个词的向量一起进入网络（cat或add）\n",
    "3. cat（最后一维）的话，需要截断，或者另外卷积一次获得新hadden_input\n",
    "4. pytorch的rnn为x[i]通过一liner，hidden通过一liner，两个结果相加经Tanh激活，结果作为x[i]和新hidden，具体见下面代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before embedding:torch.Size([4, 20])\n",
      "after embedding:torch.Size([4, 20, 8])\n",
      "-------------\n",
      "rnn input:torch.Size([20, 4, 8])\n",
      "rnn out:torch.Size([20, 4, 128])\n",
      "rnn outh:torch.Size([4, 128])\n",
      "-----------\n",
      "finally shape:torch.Size([4, 2])\n",
      "tensor([[0.5058, 0.4942],\n",
      "        [0.5616, 0.4384],\n",
      "        [0.4322, 0.5678],\n",
      "        [0.4265, 0.5735]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# rnn内部细节\n",
    "batch_size = 4\n",
    "sentence_len = 20 #一句话20个词\n",
    "words_num = 100 #字典记录了100个词\n",
    "words_len = 8 #字典的每个词向量长8\n",
    "X = torch.randint(0,words_num,[batch_size,sentence_len]) #注意输入不为one-hot，只是每个词的字典序号，比如[3,2,10]表示一句话。int\n",
    "y = torch.as_tensor([[0.,1],[0,1],[1,0],[1,0]])\n",
    "#print(X)\n",
    "\n",
    "hidden_layer_num = 1 #多少个hidden用于循环，即多少个循环部分\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = 128\n",
    "        self.embedding_layer = nn.Embedding(words_num,words_len,padding_idx=0) #100*8,padding_idx为输入长度不够时填充的字典词序号\n",
    "\n",
    "        #一个循环节\n",
    "        self.hidden_layer_x = nn.Linear(words_len,self.hidden_size) #rnn关键部分\n",
    "        self.hidden_layer_h = nn.Linear(self.hidden_size,self.hidden_size) #给h用\n",
    "\n",
    "        #分类器\n",
    "        self.out_layer = nn.Linear(self.hidden_size,2)\n",
    "        self.activation_layer = nn.Softmax(dim=-1) #dim=0表示a[i][j][k]按i方向的几个数一起算\n",
    "\n",
    "    def __init_hidden(self):\n",
    "        return torch.zeros([hidden_layer_num,batch_size,self.hidden_size])\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.hidden = self.__init_hidden()\n",
    "\n",
    "        print(f\"before embedding:{x.shape}\") #[4, 20]\n",
    "        x = self.embedding_layer(x)\n",
    "        print(f\"after embedding:{x.shape}\") #[4, 20, 8]\n",
    "        print(\"-------------\")\n",
    "\n",
    "        out = torch.zeros([sentence_len, batch_size, self.hidden_size])\n",
    "\n",
    "        #rnn部分\n",
    "        # 为了更好计算，将数据x变形为为len_sencentces*batch_size*words_num\n",
    "        # 即x[0]为各句子首单词\n",
    "        x = x.transpose(0,1)\n",
    "        print(f\"rnn input:{x.shape}\") #[20, 4, 128]\n",
    "        for i in range(x.shape[0]):\n",
    "            a1 = self.hidden_layer_x(x[i])\n",
    "            a2 = self.hidden_layer_h(self.hidden[0])\n",
    "\n",
    "            out[i] = self.hidden[0] = nn.Tanh()(a1+a2)\n",
    "        print(f\"rnn out:{out.shape}\")\n",
    "        print(f\"rnn outh:{self.hidden[0].shape}\")\n",
    "        print(\"-----------\")\n",
    "        \n",
    "        #分类器\n",
    "        o = self.hidden[0]\n",
    "        o = self.out_layer(o)\n",
    "        o = self.activation_layer(o)\n",
    "        print(f\"finally shape:{o.shape}\") #[4, 2]\n",
    "        print(o)\n",
    "\n",
    "        return o\n",
    "\n",
    "rnn = MyRNN()\n",
    "out = rnn(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch的rnn\n",
    "- 首先就是输入是batch_size\\*len_sencentces\\*words_num  \n",
    "所以Embedding算是预处理部分，如果需要训练则？？？？？？？？\n",
    "- 主要公式$$h_t = \\tanh(x_t W_{ih}^T + b_{ih} + h_{t-1}W_{hh}^T + b_{hh})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 8])\n",
      "torch.Size([2, 5, 32]) torch.Size([1, 2, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "sentence_len = 5\n",
    "words_num = 10\n",
    "words_len = 8\n",
    "\n",
    "X = torch.randint(0,words_num,[batch_size,sentence_len])\n",
    "\n",
    "X = nn.Embedding(words_num,words_len)(X)\n",
    "#X = X.transpose(0,1) #batch_size放在第二维，则batch_first设置为False\n",
    "print(X.shape)\n",
    "\n",
    "hidden_size=32\n",
    "num_layers=1\n",
    "H = torch.zeros([num_layers,batch_size,hidden_size]) #可以不设置，则默认为0，这是单向rnn\n",
    "#HH = torch.zeros([num_layers*2,batch_size,hidden_size]) #双向rnn，需要RNN中设置bidirectional=True\n",
    "\n",
    "rnn = nn.RNN(\n",
    "            input_size=words_len,\n",
    "\n",
    "            #hidden_size，num_layers 都是对网络的设置，与输入数据无关，设置相对自由\n",
    "            hidden_size=hidden_size,     # hidden层大小\n",
    "            num_layers=num_layers,       # n个rnn层\n",
    "\n",
    "            batch_first=True, #True则输入输出的batch在第一维，否则在第二维（参照上面MyRNN在hidden前的变形）\n",
    "\n",
    "            bidirectional=False, #是否双向rnn\n",
    "        )\n",
    "\n",
    "out,outh = rnn(X,H) #out为h的集合\n",
    "print(out.shape,outh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([32, 8]) <class 'torch.nn.parameter.Parameter'>\n",
      "weight_hh_l0 torch.Size([32, 32]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias_ih_l0 torch.Size([32]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias_hh_l0 torch.Size([32]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for i in rnn.named_parameters():\n",
    "    print(i[0],i[1].shape,type(i[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 4])\n",
      "pytorch部分\n",
      "torch.Size([2, 5, 8]) torch.Size([1, 2, 8])\n",
      "tensor([-0.7137,  0.5926, -0.6244,  0.6198, -0.1706, -0.5207, -0.0612,  0.4735],\n",
      "       grad_fn=<SliceBackward0>) \n",
      " tensor([-0.8456,  0.2111,  0.1764,  0.7712, -0.0146, -0.7987, -0.6467,  0.7652],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "-------------------\n",
      "MyCNN部分\n",
      "torch.Size([2, 5, 8]) torch.Size([1, 2, 8])\n",
      "tensor([-0.7137,  0.5926, -0.6244,  0.6198, -0.1706, -0.5207, -0.0612,  0.4735],\n",
      "       grad_fn=<SliceBackward0>) \n",
      " tensor([-0.8456,  0.2111,  0.1764,  0.7712, -0.0146, -0.7987, -0.6467,  0.7652],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1024)\n",
    "\n",
    "batch_size = 2\n",
    "sentence_len = 5\n",
    "words_num = 10\n",
    "words_len = 4\n",
    "\n",
    "hidden_size=8\n",
    "num_layers=1\n",
    "\n",
    "X = torch.randint(0,words_num,(batch_size,sentence_len))\n",
    "em = nn.Embedding(words_num,words_len)\n",
    "X = em(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('pytorch部分')\n",
    "rnn = nn.RNN(\n",
    "            input_size=words_len,\n",
    "\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "\n",
    "            batch_first=True,\n",
    "        )\n",
    "out,outh = rnn(X)\n",
    "print(out.shape,outh.shape)\n",
    "print(outh[0,0,:],'\\n',out[0,0,:])\n",
    "print('-------------------')\n",
    "\n",
    "\n",
    "print('MyCNN部分')\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.hidden_layer_x = nn.Linear(words_len,hidden_size) #rnn关键部分\n",
    "        self.hidden_layer_h = nn.Linear(hidden_size,hidden_size) #h用\n",
    "\n",
    "        #注意这里无条件复制w、b，所以最好先判断两者形状相等再赋值\n",
    "        params = list(rnn.parameters())\n",
    "        assert self.hidden_layer_x.weight.shape == params[0].shape and \\\n",
    "                self.hidden_layer_h.weight.shape == params[1].shape and \\\n",
    "                self.hidden_layer_x.bias.shape == params[2].shape and \\\n",
    "                self.hidden_layer_h.bias.shape == params[3].shape,\\\n",
    "                print(\"shape error\")\n",
    "\n",
    "        self.hidden_layer_x.weight = params[0]\n",
    "        self.hidden_layer_h.weight = params[1]\n",
    "        self.hidden_layer_x.bias = params[2]\n",
    "        self.hidden_layer_h.bias = params[3]\n",
    "\n",
    "    def __init_hidden(self):\n",
    "        return torch.zeros([num_layers,batch_size,hidden_size])\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.hidden = self.__init_hidden()\n",
    "\n",
    "        out = torch.zeros([sentence_len,batch_size,hidden_size])\n",
    "\n",
    "        x = x.transpose(0,1)\n",
    "        for i in range(x.shape[0]):\n",
    "            a1 = self.hidden_layer_x(x[i])\n",
    "            a2 = self.hidden_layer_h(self.hidden[0])\n",
    "            self.hidden[0] = nn.Tanh()(a1+a2)\n",
    "\n",
    "\n",
    "            out[i] = self.hidden[0]\n",
    "\n",
    "        return out.transpose(0,1)\n",
    "\n",
    "myrnn = MyRNN()\n",
    "out = myrnn(X)\n",
    "print(out.shape,myrnn.hidden.shape)\n",
    "print(myrnn.hidden[0,0,:],'\\n',out[0,0,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "## 基本理解\n",
    "- rnn因为激活层是Tanh，显然，当前信息几乎不会对很远的计算产生影响，即只能短期记忆\n",
    "- lstm中引入可以选择“记忆”和“当前”的信息对当前输出的占比 \n",
    "## 具体介绍 \n",
    "- 输入是H（类似于rnn的hidden作用），C（当前记忆）\n",
    "- 一个单元分为记忆门，遗忘门，rnn门，输出门，其相当于四次rnn变换，只不过功能和激活函数不同\n",
    "   - 记忆门i，Sigmoid激活，即当前rnn输出需要记哪些信息到记忆C中\n",
    "   - 遗忘门f，Sigmoid激活，即原先记忆需要遗忘哪些信息\n",
    "   - rnn门g，Tanh激活，就是rnn\n",
    "   - 输出门o，Sigmoid激活，控制新记忆哪些作为新H\n",
    "- 公式如下\n",
    "$$\n",
    "    \\begin{array}{ll} \\\\\n",
    "        i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "        f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "        o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "        c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "        h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "    \\end{array}\n",
    "$$\n",
    "## pytorch内部细节\n",
    "- 将四个变换按维度一放在一起，同时计算四个门激活前的输出，输出顺序如上顺序，见如下代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 4])\n",
      "pytorch部分\n",
      "torch.Size([2, 5, 16]) torch.Size([1, 2, 16]) torch.Size([1, 2, 16])\n",
      "tensor([ 0.0842,  0.0372, -0.1389,  0.0300, -0.1425,  0.0223,  0.0017,  0.3239,\n",
      "        -0.0413, -0.0924, -0.1835,  0.0621,  0.0484, -0.0894,  0.2133,  0.0063],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "-------------------\n",
      "MyLSTM部分\n",
      "torch.Size([2, 5, 16]) torch.Size([1, 2, 16]) torch.Size([1, 2, 16])\n",
      "tensor([ 0.0842,  0.0372, -0.1389,  0.0300, -0.1425,  0.0223,  0.0017,  0.3239,\n",
      "        -0.0413, -0.0924, -0.1835,  0.0621,  0.0484, -0.0894,  0.2133,  0.0063],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1024)\n",
    "\n",
    "batch_size = 2\n",
    "sentence_len = 5\n",
    "words_num = 10\n",
    "words_len = 4\n",
    "\n",
    "hidden_size=16\n",
    "num_layers=1\n",
    "\n",
    "X = torch.randint(0,words_num,(batch_size,sentence_len))\n",
    "em = nn.Embedding(words_num,words_len)\n",
    "X = em(X)\n",
    "print(X.shape)\n",
    "\n",
    "print('pytorch部分')\n",
    "lstm = nn.LSTM(\n",
    "            input_size=words_len,\n",
    "\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "\n",
    "            batch_first=True,\n",
    "        )\n",
    "H = torch.zeros([num_layers,batch_size,hidden_size])\n",
    "C = torch.zeros([num_layers,batch_size,hidden_size])\n",
    "out,(outh,outc) = lstm(X,(H,C)) #H、C默认为0\n",
    "print(out.shape,outh.shape,outc.shape)\n",
    "print(out[0,0,:])\n",
    "print('-------------------')\n",
    "\n",
    "\n",
    "print('MyLSTM部分')\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layer_x = nn.Linear(words_len,hidden_size*4) #4*32，即同时输出4组\n",
    "        self.hidden_layer_h = nn.Linear(hidden_size,hidden_size*4) #8*32\n",
    "\n",
    "        params = list(lstm.parameters())\n",
    "        assert self.hidden_layer_x.weight.shape == params[0].shape and \\\n",
    "                self.hidden_layer_h.weight.shape == params[1].shape and \\\n",
    "                self.hidden_layer_x.bias.shape == params[2].shape and \\\n",
    "                self.hidden_layer_h.bias.shape == params[3].shape, \\\n",
    "                print(\"shape error\")\n",
    "\n",
    "        self.hidden_layer_x.weight = params[0]\n",
    "        self.hidden_layer_h.weight = params[1]\n",
    "        self.hidden_layer_x.bias = params[2]\n",
    "        self.hidden_layer_h.bias = params[3]\n",
    "\n",
    "    def __init_H_C(self):\n",
    "        return torch.zeros([num_layers,batch_size,hidden_size]),torch.zeros([num_layers,batch_size,hidden_size])\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.H,self.C = self.__init_H_C()\n",
    "        #H是输出\n",
    "        #C是记忆\n",
    "\n",
    "        out = torch.zeros([sentence_len,batch_size,hidden_size])\n",
    "\n",
    "        x = x.transpose(0,1)\n",
    "        for i in torch.arange(x.shape[0]):\n",
    "\n",
    "            a1 = self.hidden_layer_x(x[i])\n",
    "            a2 = self.hidden_layer_h(self.H[0])\n",
    "            a = a1+a2\n",
    "\n",
    "            remember_gate = nn.Sigmoid()(a[:,:hidden_size*1]) #记住下面op什么信息\n",
    "            forget_gate = nn.Sigmoid()(a[:,hidden_size*1:hidden_size*2]) #C遗忘什么信息\n",
    "            op = nn.Tanh()(a[:,hidden_size*2:hidden_size*3])\n",
    "            output_gate = nn.Sigmoid()(a[:,hidden_size*3:]) #最终输出\n",
    "\t\t\t\n",
    "            self.C[0] = self.C[0]*forget_gate + remember_gate*op #新的记忆\n",
    "            self.H[0] = nn.Tanh()(self.C)*output_gate #生成新的输出\n",
    "\n",
    "            out[i] = self.H[0]\n",
    "\n",
    "        return out.transpose(0,1)\n",
    "\n",
    "mylstm = MyLSTM()\n",
    "out = mylstm(X)\n",
    "print(out.shape,mylstm.C.shape,mylstm.H.shape)\n",
    "print(out[0,0,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化\n",
    "- 遗忘门=1-记忆门，减少运算次数\n",
    "- 。。。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformers包"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4115338fe7ab4a538fea3ef25fde7a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\project\\python\\feedback-prize-effectiveness\\exp.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m question_answerer \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mquestion-answering\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m#内部有模型，用于回答问题的\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mExtractive Question Answering is the task of extracting an answer from a text given a question. An example of a\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mquestion answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39ma model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m question_answerer(question\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWhat is extractive question answering?\u001b[39m\u001b[39m\"\u001b[39m, context\u001b[39m=\u001b[39mcontext)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\pipelines\\__init__.py:724\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    723\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 724\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    725\u001b[0m     model,\n\u001b[0;32m    726\u001b[0m     model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    727\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    728\u001b[0m     framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    729\u001b[0m     task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    730\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    731\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    732\u001b[0m )\n\u001b[0;32m    734\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    735\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\pipelines\\base.py:257\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m     )\n\u001b[0;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 257\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39mfrom_pretrained(model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    258\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    259\u001b[0m         model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    462\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    465\u001b[0m     )\n\u001b[0;32m    466\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py:2137\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2123\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2124\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m   2125\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2126\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2135\u001b[0m         _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   2136\u001b[0m     )\n\u001b[1;32m-> 2137\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   2139\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2140\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m SAFE_WEIGHTS_NAME:\n\u001b[0;32m   2142\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[0;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[0;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[0;32m    122\u001b[0m     )\n\u001b[1;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:1242\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[0;32m   1240\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[1;32m-> 1242\u001b[0m     http_get(\n\u001b[0;32m   1243\u001b[0m         url_to_download,\n\u001b[0;32m   1244\u001b[0m         temp_file,\n\u001b[0;32m   1245\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1246\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1247\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1248\u001b[0m     )\n\u001b[0;32m   1250\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, blob_path)\n\u001b[0;32m   1251\u001b[0m _chmod_and_replace(temp_file\u001b[39m.\u001b[39mname, blob_path)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:495\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[0;32m    486\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    487\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    488\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    489\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    494\u001b[0m )\n\u001b[1;32m--> 495\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[0;32m    496\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    497\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py:627\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 627\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    629\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    630\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py:566\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    563\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    565\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 566\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py:532\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    530\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\") #内部有模型，用于回答问题的\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\"\n",
    "question_answerer(question=\"What is extractive question answering?\", context=context)\n",
    "\n",
    "#中文版\n",
    "from transformers import AutoModelForQuestionAnswering,AutoTokenizer,pipeline\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('uer/roberta-base-chinese-extractive-qa')\n",
    "tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-chinese-extractive-qa')\n",
    "zh_qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "QA_input = {\n",
    "    'question': \"著名诗歌《假如生活欺骗了你》的作者是\",\n",
    "    'context': \"普希金从那里学习人民的语言，吸取了许多有益的养料，\\\n",
    "        这一切对普希金后来的创作产生了很大的影响。这两年里，普希金创作了不少优秀的作品，如《囚徒》、\\\n",
    "            《致大海》、《致凯恩》和《假如生活欺骗了你》等几十首抒情诗，叙事诗《努林伯爵》，历史剧\\\n",
    "                《鲍里斯·戈都诺夫》，以及《叶甫盖尼·奥涅金》前六章。\"\n",
    "                }\n",
    "zh_qa(QA_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你', '好', '吗', '，', '吃', '饭', '了', '吗', '?']\n",
      "{'input_ids': [[101, 5031, 3428, 3221, 679, 7444, 6206, 102, 0, 0, 0, 0, 0, 0, 0], [101, 2130, 1059, 679, 7444, 6206, 818, 862, 7583, 1912, 3082, 868, 102, 0, 0], [101, 1914, 3340, 3144, 2945, 1469, 1296, 3340, 3144, 2945, 671, 3416, 6822, 6121, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\") #https://huggingface.co/有很多模型\n",
    "sen = \"你好吗，吃饭了吗?\"\n",
    "tokens = tokenizer.tokenize(sen)\n",
    "print(tokens)\n",
    "\n",
    "sens = [\"答案是不需要\",\"完全不需要任何额外操作\",\"多条数据和单条数据一样进行调用即可.\"]\n",
    "res = tokenizer(\n",
    "    sens, \n",
    "    padding=\"max_length\", #不足补齐\n",
    "    max_length=15,\n",
    "    truncation=True #超过截断\n",
    "    )\n",
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本预处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"E:/DATA/feedback-prize-effectiveness/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9704a709b505</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>On my perspective, I think that the face is a ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c22adee811b6</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>I think that the face is a natural landform be...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a10d361e54e4</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>If life was on Mars, we would know by now. The...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db3e453ec4e2</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>People thought that the face was formed by ali...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id      essay_id  \\\n",
       "0  0013cc385424  007ACE74B050   \n",
       "1  9704a709b505  007ACE74B050   \n",
       "2  c22adee811b6  007ACE74B050   \n",
       "3  a10d361e54e4  007ACE74B050   \n",
       "4  db3e453ec4e2  007ACE74B050   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Hi, i'm Isaac, i'm going to be writing about h...           Lead   \n",
       "1  On my perspective, I think that the face is a ...       Position   \n",
       "2  I think that the face is a natural landform be...          Claim   \n",
       "3  If life was on Mars, we would know by now. The...       Evidence   \n",
       "4  People thought that the face was formed by ali...   Counterclaim   \n",
       "\n",
       "  discourse_effectiveness  \n",
       "0                Adequate  \n",
       "1                Adequate  \n",
       "2                Adequate  \n",
       "3                Adequate  \n",
       "4                Adequate  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(base_dir+\"/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_type             [Lead, Position, Claim, Evidence, Counterclaim...\n",
       "discourse_effectiveness                   [Adequate, Ineffective, Effective]\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['discourse_type','discourse_effectiveness']].apply(pd.unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4191\n",
      "4191\n"
     ]
    }
   ],
   "source": [
    "print(len(pd.unique(df['essay_id'])))\n",
    "for _,_,files in os.walk(base_dir+\"/train\"):\n",
    "    print(len(files))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8790, 117, 178, 112, 182, 7026, 117, 178, 112, 182, 1280, 1106, 1129, 2269, 1164, 1293, 1142, 1339, 1113, 7403, 1110, 170, 2379, 1657, 13199, 1137, 1191, 1175, 1110, 1297, 1113, 7403, 1115, 1189, 1122, 119, 1109, 1642, 1110, 1164, 1293, 9085, 1261, 170, 3439, 1104, 7403, 1105, 170, 1339, 1108, 1562, 1113, 1103, 5015, 119, 9085, 2144, 112, 189, 1221, 1191, 1103, 1657, 13199, 1108, 1687, 1118, 1297, 1113, 7403, 117, 1137, 1191, 1122, 1110, 1198, 170, 2379, 1657, 13199, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = \"E:/DATA/feedback-prize-effectiveness/\"\n",
    "df = pd.read_csv(base_dir+\"/train.csv\")\n",
    "\n",
    "needed_col = ['essay_id','discourse_text','discourse_type','discourse_effectiveness']\n",
    "df = df[needed_col]\n",
    "df.columns=['id','text','type','ef']\n",
    "\n",
    "typ = {'Lead':1,'Position':2, 'Claim':3, 'Evidence':4, 'Counterclaim':5, 'Rebuttal':6, 'Concluding Statement':7}\n",
    "eff = {'Adequate':1, 'Ineffective':2, 'Effective':3}\n",
    "df['type'] = df['type'].apply(lambda x:typ[x])\n",
    "df['ef'] = df['ef'].apply(lambda x:eff[x])\n",
    "\n",
    "#text处理\n",
    "tk = AutoTokenizer.from_pretrained('bert-base-cased',use_fast=True)\n",
    "tk.max_len=384\n",
    "df['text'] = df['text'].apply(lambda x:tk(x,truncation=True))\n",
    "\n",
    "df.iloc[0,1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "modules_dir已存在，已将其清空\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "base_dir = \"E:/DATA/feedback-prize-effectiveness/\"\n",
    "modules_dir = './modules/'\n",
    "if not os.path.exists(modules_dir):\n",
    "    os.mkdir(modules_dir)\n",
    "    print('modules_dir已创建')\n",
    "else:\n",
    "    for i in os.listdir(modules_dir):\n",
    "        c_path = os.path.join(modules_dir, i)\n",
    "        os.remove(c_path)\n",
    "    print('modules_dir已存在，已将其清空')\n",
    "\n",
    "input_len = 384\n",
    "\n",
    "train_batch_size=64\n",
    "test_batch_size=128\n",
    "epochs=15\n",
    "\n",
    "seed=101\n",
    "def set_seed():\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed()\n",
    "\n",
    "exp_name='exp1'\n",
    "import shutil\n",
    "if os.path.exists(exp_name):\n",
    "    shutil.rmtree(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mode='debug'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = AutoTokenizer.from_pretrained('bert-base-cased',use_fast=True)\n",
    "tk.max_len=input_len\n",
    "\n",
    "class DfProc():\n",
    "    def __init__(self, data_mode='debug'):\n",
    "\n",
    "        self.data_mode = data_mode\n",
    "        assert self.data_mode in ['debug','mini','all'],print(\"mode值错误\")\n",
    "\n",
    "        self.__Set_Df()\n",
    "\n",
    "\n",
    "    def __Set_Df(self):\n",
    "\n",
    "        #train\n",
    "        df = pd.read_csv(base_dir+\"/train.csv\")\n",
    "\n",
    "        #这里的策略是将type列作为输入文本的开头，则type列不再需要\n",
    "        df['discourse_text'] = df['discourse_type']+ tk.sep_token + df['discourse_text'] #特殊符号\n",
    "\n",
    "        #skf = StratifiedGroupKFold(5)\n",
    "        #for i, (train_i,valid_i) in enumerate(skf.split(df,df['discourse_type'],groups=df['essay_id'])):\n",
    "        #    df.loc[valid_i,'fold'] = i+1\n",
    "\n",
    "\n",
    "        needed_col = ['essay_id','discourse_text','discourse_effectiveness']\n",
    "        df = df[needed_col]\n",
    "        df.columns=['id','text','ef']\n",
    "\n",
    "        eff = {'Adequate':1, 'Ineffective':2, 'Effective':3}\n",
    "        df['ef'] = df['ef'].apply(lambda x:eff[x])\n",
    "\n",
    "        #text处理\n",
    "        df['text'] = df['text'].apply(lambda x:tk(x,truncation=True))\n",
    "\n",
    "        #train_df = df[df['fold']!=1]\n",
    "        #valid_df = df[df['fold']==1]\n",
    "        train_df = df\n",
    "\n",
    "\n",
    "        if self.data_mode == 'all':\n",
    "            self.train_df = train_df\n",
    "            #self.valid_df = valid_df\n",
    "        elif self.data_mode == 'debug':\n",
    "            self.train_df = train_df[:2*train_batch_size]\n",
    "            #self.valid_df = valid_df[:2*test_batch_size]\n",
    "        else:\n",
    "            self.train_df = train_df[:int(0.3*len(train_df))]\n",
    "            #self.valid_df = valid_df[:int(0.3*len(valid_df))]\n",
    "\n",
    "        #test\n",
    "        df = pd.read_csv(base_dir+\"/test.csv\")\n",
    "\n",
    "        df['discourse_text'] = df['discourse_type']+ tk.sep_token + df['discourse_text']\n",
    "\n",
    "        needed_col = ['essay_id', 'discourse_text']\n",
    "        df = df[needed_col]\n",
    "        df.columns=['id','text']\n",
    "\n",
    "        df['text'] = df['text'].apply(lambda x:tk(x,truncation=True))\n",
    "\n",
    "        if self.data_mode == 'all':\n",
    "            self.test_df = df\n",
    "        elif self.data_mode == 'debug':\n",
    "            self.test_df = df[:2*test_batch_size]\n",
    "        else:\n",
    "            self.test_df = df[:int(0.3*len(df))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d233a11d4546c6b43e78d97b88728d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\project\\python\\feedback-prize-effectiveness\\exp.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mbert-base-cased\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         num_labels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         model, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         args, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         compute_metrics\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m prey,y:nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()(prey,y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m trainer \u001b[39m=\u001b[39m get_trainer(DfProc())\n",
      "\u001b[1;32me:\\project\\python\\feedback-prize-effectiveness\\exp.ipynb Cell 31\u001b[0m in \u001b[0;36mget_trainer\u001b[1;34m(dfProc)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_trainer\u001b[39m(dfProc):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         learning_rate\u001b[39m=\u001b[39m\u001b[39m8e-5\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         report_to\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mbert-base-cased\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         num_labels\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         model, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         args, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         compute_metrics\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m prey,y:nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()(prey,y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project/python/feedback-prize-effectiveness/exp.ipynb#X51sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    462\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    465\u001b[0m     )\n\u001b[0;32m    466\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py:2137\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2123\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2124\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m   2125\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2126\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2135\u001b[0m         _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   2136\u001b[0m     )\n\u001b[1;32m-> 2137\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   2139\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2140\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m SAFE_WEIGHTS_NAME:\n\u001b[0;32m   2142\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[0;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[0;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[0;32m    122\u001b[0m     )\n\u001b[1;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:1242\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[0;32m   1240\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[1;32m-> 1242\u001b[0m     http_get(\n\u001b[0;32m   1243\u001b[0m         url_to_download,\n\u001b[0;32m   1244\u001b[0m         temp_file,\n\u001b[0;32m   1245\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1246\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1247\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1248\u001b[0m     )\n\u001b[0;32m   1250\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, blob_path)\n\u001b[0;32m   1251\u001b[0m _chmod_and_replace(temp_file\u001b[39m.\u001b[39mname, blob_path)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:495\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[0;32m    486\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    487\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    488\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    489\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    494\u001b[0m )\n\u001b[1;32m--> 495\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[0;32m    496\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    497\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py:627\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 627\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    629\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    630\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py:566\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    563\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    565\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 566\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\urllib3\\response.py:532\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    530\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_trainer(dfProc):\n",
    "    args = TrainingArguments(\n",
    "        'outputs', \n",
    "        learning_rate=8e-5, \n",
    "        warmup_ratio=0.1, \n",
    "        lr_scheduler_type='cosine', \n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\", \n",
    "        per_device_train_batch_size=train_batch_size, \n",
    "        per_device_eval_batch_size=test_batch_size,\n",
    "        num_train_epochs=epochs, \n",
    "        weight_decay=0.01, \n",
    "        report_to='none'\n",
    "        )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'bert-base-cased', \n",
    "        num_labels=3\n",
    "        )\n",
    "    return Trainer(\n",
    "        model, \n",
    "        args, \n",
    "        train_dataset=dfProc.train_df, \n",
    "        eval_dataset=dfProc.test_df,\n",
    "        tokenizer=tk, \n",
    "        compute_metrics=lambda prey,y:nn.BCEWithLogitsLoss()(prey,y)\n",
    "        )\n",
    "trainer = get_trainer(DfProc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
